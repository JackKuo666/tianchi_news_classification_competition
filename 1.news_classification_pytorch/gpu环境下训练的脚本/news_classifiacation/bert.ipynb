{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-20 10:50:24,196 INFO: Use cuda: True, gpu id: 0.\n",
      "2021-07-20 10:50:29,951 INFO: Fold lens [20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]\n",
      "2021-07-20 10:51:21,462 INFO: Build vocab: words 5978, labels 14.\n",
      "2021-07-20 10:51:21,507 INFO: Build Bert vocab with size 5981.\n",
      "Some weights of the model checkpoint at /home/featurize/data/bert-mini/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2021-07-20 10:51:21,608 INFO: Build Bert encoder with pooled False.\n",
      "2021-07-20 10:51:21,627 INFO: Use cuda: True, gpu id: 0.\n",
      "2021-07-20 10:51:24,934 INFO: Build model with bert word encoder, lstm sent encoder.\n",
      "2021-07-20 10:51:24,935 INFO: Model param num: 7.72 M.\n",
      "2021-07-20 11:10:10,870 INFO: Total 180000 docs.\n",
      "2021-07-20 11:12:15,310 INFO: Total 20000 docs.\n",
      "2021-07-20 11:17:29,670 INFO: Total 50000 docs.\n",
      "2021-07-20 11:17:29,671 INFO: Start training...\n",
      "/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "2021-07-20 11:17:46,783 INFO: | epoch   1 | step  50 | batch  50/11250 | lr 0.00020 0.00005 | loss 2.0274 | s/batch 0.34\n",
      "2021-07-20 11:18:01,502 INFO: | epoch   1 | step 100 | batch 100/11250 | lr 0.00020 0.00005 | loss 1.1057 | s/batch 0.29\n",
      "2021-07-20 11:18:16,891 INFO: | epoch   1 | step 150 | batch 150/11250 | lr 0.00020 0.00005 | loss 0.9477 | s/batch 0.31\n",
      "2021-07-20 11:18:30,711 INFO: | epoch   1 | step 200 | batch 200/11250 | lr 0.00020 0.00005 | loss 0.7772 | s/batch 0.28\n",
      "2021-07-20 11:18:45,844 INFO: | epoch   1 | step 250 | batch 250/11250 | lr 0.00020 0.00005 | loss 0.6905 | s/batch 0.30\n",
      "2021-07-20 11:18:59,549 INFO: | epoch   1 | step 300 | batch 300/11250 | lr 0.00020 0.00005 | loss 0.5398 | s/batch 0.27\n",
      "2021-07-20 11:19:10,737 INFO: | epoch   1 | step 350 | batch 350/11250 | lr 0.00020 0.00005 | loss 0.4915 | s/batch 0.22\n",
      "2021-07-20 11:19:25,821 INFO: | epoch   1 | step 400 | batch 400/11250 | lr 0.00020 0.00005 | loss 0.5598 | s/batch 0.30\n",
      "2021-07-20 11:19:41,196 INFO: | epoch   1 | step 450 | batch 450/11250 | lr 0.00020 0.00005 | loss 0.4838 | s/batch 0.31\n",
      "2021-07-20 11:19:53,756 INFO: | epoch   1 | step 500 | batch 500/11250 | lr 0.00020 0.00005 | loss 0.4401 | s/batch 0.25\n",
      "2021-07-20 11:20:09,446 INFO: | epoch   1 | step 550 | batch 550/11250 | lr 0.00020 0.00005 | loss 0.4028 | s/batch 0.31\n",
      "2021-07-20 11:20:24,644 INFO: | epoch   1 | step 600 | batch 600/11250 | lr 0.00020 0.00005 | loss 0.5179 | s/batch 0.30\n",
      "2021-07-20 11:20:38,550 INFO: | epoch   1 | step 650 | batch 650/11250 | lr 0.00020 0.00005 | loss 0.4069 | s/batch 0.28\n",
      "2021-07-20 11:20:53,414 INFO: | epoch   1 | step 700 | batch 700/11250 | lr 0.00020 0.00005 | loss 0.3783 | s/batch 0.30\n",
      "2021-07-20 11:21:07,300 INFO: | epoch   1 | step 750 | batch 750/11250 | lr 0.00020 0.00005 | loss 0.3257 | s/batch 0.28\n",
      "2021-07-20 11:21:23,305 INFO: | epoch   1 | step 800 | batch 800/11250 | lr 0.00020 0.00005 | loss 0.3797 | s/batch 0.32\n",
      "2021-07-20 11:21:36,167 INFO: | epoch   1 | step 850 | batch 850/11250 | lr 0.00020 0.00005 | loss 0.3450 | s/batch 0.26\n",
      "2021-07-20 11:21:50,793 INFO: | epoch   1 | step 900 | batch 900/11250 | lr 0.00020 0.00005 | loss 0.4346 | s/batch 0.29\n",
      "2021-07-20 11:22:03,473 INFO: | epoch   1 | step 950 | batch 950/11250 | lr 0.00020 0.00005 | loss 0.3916 | s/batch 0.25\n",
      "2021-07-20 11:22:16,387 INFO: | epoch   1 | step 1000 | batch 1000/11250 | lr 0.00015 0.00005 | loss 0.2936 | s/batch 0.26\n",
      "2021-07-20 11:22:29,723 INFO: | epoch   1 | step 1050 | batch 1050/11250 | lr 0.00015 0.00005 | loss 0.2978 | s/batch 0.27\n",
      "2021-07-20 11:22:43,791 INFO: | epoch   1 | step 1100 | batch 1100/11250 | lr 0.00015 0.00005 | loss 0.3702 | s/batch 0.28\n",
      "2021-07-20 11:22:55,455 INFO: | epoch   1 | step 1150 | batch 1150/11250 | lr 0.00015 0.00004 | loss 0.3403 | s/batch 0.23\n",
      "2021-07-20 11:23:08,711 INFO: | epoch   1 | step 1200 | batch 1200/11250 | lr 0.00015 0.00004 | loss 0.3370 | s/batch 0.27\n",
      "2021-07-20 11:23:21,797 INFO: | epoch   1 | step 1250 | batch 1250/11250 | lr 0.00015 0.00004 | loss 0.2944 | s/batch 0.26\n",
      "2021-07-20 11:23:36,884 INFO: | epoch   1 | step 1300 | batch 1300/11250 | lr 0.00015 0.00004 | loss 0.3106 | s/batch 0.30\n",
      "2021-07-20 11:23:52,667 INFO: | epoch   1 | step 1350 | batch 1350/11250 | lr 0.00015 0.00004 | loss 0.3179 | s/batch 0.32\n",
      "2021-07-20 11:24:07,002 INFO: | epoch   1 | step 1400 | batch 1400/11250 | lr 0.00015 0.00004 | loss 0.4002 | s/batch 0.29\n",
      "2021-07-20 11:24:21,234 INFO: | epoch   1 | step 1450 | batch 1450/11250 | lr 0.00015 0.00004 | loss 0.3037 | s/batch 0.28\n",
      "2021-07-20 11:24:34,466 INFO: | epoch   1 | step 1500 | batch 1500/11250 | lr 0.00015 0.00004 | loss 0.3429 | s/batch 0.26\n",
      "2021-07-20 11:24:48,957 INFO: | epoch   1 | step 1550 | batch 1550/11250 | lr 0.00015 0.00004 | loss 0.3491 | s/batch 0.29\n",
      "2021-07-20 11:25:03,594 INFO: | epoch   1 | step 1600 | batch 1600/11250 | lr 0.00015 0.00004 | loss 0.3789 | s/batch 0.29\n",
      "2021-07-20 11:25:17,630 INFO: | epoch   1 | step 1650 | batch 1650/11250 | lr 0.00015 0.00004 | loss 0.2841 | s/batch 0.28\n",
      "2021-07-20 11:25:29,747 INFO: | epoch   1 | step 1700 | batch 1700/11250 | lr 0.00015 0.00004 | loss 0.3138 | s/batch 0.24\n",
      "2021-07-20 11:25:46,378 INFO: | epoch   1 | step 1750 | batch 1750/11250 | lr 0.00015 0.00004 | loss 0.3297 | s/batch 0.33\n",
      "2021-07-20 11:26:01,360 INFO: | epoch   1 | step 1800 | batch 1800/11250 | lr 0.00015 0.00004 | loss 0.3513 | s/batch 0.30\n",
      "2021-07-20 11:26:15,442 INFO: | epoch   1 | step 1850 | batch 1850/11250 | lr 0.00015 0.00004 | loss 0.2695 | s/batch 0.28\n",
      "2021-07-20 11:26:28,161 INFO: | epoch   1 | step 1900 | batch 1900/11250 | lr 0.00015 0.00004 | loss 0.3016 | s/batch 0.25\n",
      "2021-07-20 11:26:43,156 INFO: | epoch   1 | step 1950 | batch 1950/11250 | lr 0.00015 0.00004 | loss 0.2980 | s/batch 0.30\n",
      "2021-07-20 11:26:56,782 INFO: | epoch   1 | step 2000 | batch 2000/11250 | lr 0.00011 0.00004 | loss 0.2702 | s/batch 0.27\n",
      "2021-07-20 11:27:11,266 INFO: | epoch   1 | step 2050 | batch 2050/11250 | lr 0.00011 0.00004 | loss 0.2800 | s/batch 0.29\n",
      "2021-07-20 11:27:24,038 INFO: | epoch   1 | step 2100 | batch 2100/11250 | lr 0.00011 0.00004 | loss 0.2668 | s/batch 0.26\n",
      "2021-07-20 11:27:38,482 INFO: | epoch   1 | step 2150 | batch 2150/11250 | lr 0.00011 0.00004 | loss 0.2770 | s/batch 0.29\n",
      "2021-07-20 11:27:52,615 INFO: | epoch   1 | step 2200 | batch 2200/11250 | lr 0.00011 0.00004 | loss 0.2933 | s/batch 0.28\n",
      "2021-07-20 11:28:05,076 INFO: | epoch   1 | step 2250 | batch 2250/11250 | lr 0.00011 0.00004 | loss 0.2546 | s/batch 0.25\n",
      "2021-07-20 11:28:18,835 INFO: | epoch   1 | step 2300 | batch 2300/11250 | lr 0.00011 0.00004 | loss 0.2653 | s/batch 0.28\n",
      "2021-07-20 11:28:31,512 INFO: | epoch   1 | step 2350 | batch 2350/11250 | lr 0.00011 0.00004 | loss 0.2870 | s/batch 0.25\n",
      "2021-07-20 11:28:44,691 INFO: | epoch   1 | step 2400 | batch 2400/11250 | lr 0.00011 0.00004 | loss 0.2430 | s/batch 0.26\n",
      "2021-07-20 11:28:57,699 INFO: | epoch   1 | step 2450 | batch 2450/11250 | lr 0.00011 0.00004 | loss 0.2583 | s/batch 0.26\n",
      "2021-07-20 11:29:11,980 INFO: | epoch   1 | step 2500 | batch 2500/11250 | lr 0.00011 0.00004 | loss 0.2594 | s/batch 0.29\n",
      "2021-07-20 11:29:26,196 INFO: | epoch   1 | step 2550 | batch 2550/11250 | lr 0.00011 0.00004 | loss 0.2661 | s/batch 0.28\n",
      "2021-07-20 11:29:39,185 INFO: | epoch   1 | step 2600 | batch 2600/11250 | lr 0.00011 0.00004 | loss 0.2612 | s/batch 0.26\n",
      "2021-07-20 11:29:53,584 INFO: | epoch   1 | step 2650 | batch 2650/11250 | lr 0.00011 0.00004 | loss 0.2783 | s/batch 0.29\n",
      "2021-07-20 11:30:09,666 INFO: | epoch   1 | step 2700 | batch 2700/11250 | lr 0.00011 0.00004 | loss 0.3115 | s/batch 0.32\n",
      "2021-07-20 11:30:25,923 INFO: | epoch   1 | step 2750 | batch 2750/11250 | lr 0.00011 0.00004 | loss 0.2736 | s/batch 0.33\n",
      "2021-07-20 11:30:40,622 INFO: | epoch   1 | step 2800 | batch 2800/11250 | lr 0.00011 0.00004 | loss 0.2306 | s/batch 0.29\n",
      "2021-07-20 11:30:53,886 INFO: | epoch   1 | step 2850 | batch 2850/11250 | lr 0.00011 0.00004 | loss 0.3103 | s/batch 0.27\n",
      "2021-07-20 11:31:06,601 INFO: | epoch   1 | step 2900 | batch 2900/11250 | lr 0.00011 0.00004 | loss 0.2481 | s/batch 0.25\n",
      "2021-07-20 11:31:19,745 INFO: | epoch   1 | step 2950 | batch 2950/11250 | lr 0.00011 0.00004 | loss 0.2554 | s/batch 0.26\n",
      "2021-07-20 11:31:33,019 INFO: | epoch   1 | step 3000 | batch 3000/11250 | lr 0.00008 0.00004 | loss 0.2645 | s/batch 0.27\n",
      "2021-07-20 11:31:48,705 INFO: | epoch   1 | step 3050 | batch 3050/11250 | lr 0.00008 0.00004 | loss 0.2380 | s/batch 0.31\n",
      "2021-07-20 11:32:02,693 INFO: | epoch   1 | step 3100 | batch 3100/11250 | lr 0.00008 0.00004 | loss 0.2767 | s/batch 0.28\n",
      "2021-07-20 11:32:16,029 INFO: | epoch   1 | step 3150 | batch 3150/11250 | lr 0.00008 0.00004 | loss 0.3234 | s/batch 0.27\n",
      "2021-07-20 11:32:29,276 INFO: | epoch   1 | step 3200 | batch 3200/11250 | lr 0.00008 0.00004 | loss 0.1985 | s/batch 0.26\n",
      "2021-07-20 11:32:44,158 INFO: | epoch   1 | step 3250 | batch 3250/11250 | lr 0.00008 0.00004 | loss 0.2001 | s/batch 0.30\n",
      "2021-07-20 11:32:57,317 INFO: | epoch   1 | step 3300 | batch 3300/11250 | lr 0.00008 0.00004 | loss 0.2435 | s/batch 0.26\n",
      "2021-07-20 11:33:11,070 INFO: | epoch   1 | step 3350 | batch 3350/11250 | lr 0.00008 0.00004 | loss 0.2111 | s/batch 0.28\n",
      "2021-07-20 11:33:24,397 INFO: | epoch   1 | step 3400 | batch 3400/11250 | lr 0.00008 0.00003 | loss 0.1964 | s/batch 0.27\n",
      "2021-07-20 11:33:37,589 INFO: | epoch   1 | step 3450 | batch 3450/11250 | lr 0.00008 0.00003 | loss 0.2237 | s/batch 0.26\n",
      "2021-07-20 11:33:52,644 INFO: | epoch   1 | step 3500 | batch 3500/11250 | lr 0.00008 0.00003 | loss 0.3325 | s/batch 0.30\n",
      "2021-07-20 11:34:05,865 INFO: | epoch   1 | step 3550 | batch 3550/11250 | lr 0.00008 0.00003 | loss 0.2368 | s/batch 0.26\n",
      "2021-07-20 11:34:18,834 INFO: | epoch   1 | step 3600 | batch 3600/11250 | lr 0.00008 0.00003 | loss 0.2388 | s/batch 0.26\n",
      "2021-07-20 11:34:33,364 INFO: | epoch   1 | step 3650 | batch 3650/11250 | lr 0.00008 0.00003 | loss 0.3260 | s/batch 0.29\n",
      "2021-07-20 11:34:48,802 INFO: | epoch   1 | step 3700 | batch 3700/11250 | lr 0.00008 0.00003 | loss 0.2442 | s/batch 0.31\n",
      "2021-07-20 11:35:01,432 INFO: | epoch   1 | step 3750 | batch 3750/11250 | lr 0.00008 0.00003 | loss 0.2272 | s/batch 0.25\n",
      "2021-07-20 11:35:16,243 INFO: | epoch   1 | step 3800 | batch 3800/11250 | lr 0.00008 0.00003 | loss 0.2614 | s/batch 0.30\n",
      "2021-07-20 11:35:30,268 INFO: | epoch   1 | step 3850 | batch 3850/11250 | lr 0.00008 0.00003 | loss 0.2321 | s/batch 0.28\n",
      "2021-07-20 11:35:44,523 INFO: | epoch   1 | step 3900 | batch 3900/11250 | lr 0.00008 0.00003 | loss 0.2589 | s/batch 0.29\n",
      "2021-07-20 11:35:58,863 INFO: | epoch   1 | step 3950 | batch 3950/11250 | lr 0.00008 0.00003 | loss 0.2642 | s/batch 0.29\n",
      "2021-07-20 11:36:13,862 INFO: | epoch   1 | step 4000 | batch 4000/11250 | lr 0.00006 0.00003 | loss 0.1797 | s/batch 0.30\n",
      "2021-07-20 11:36:28,515 INFO: | epoch   1 | step 4050 | batch 4050/11250 | lr 0.00006 0.00003 | loss 0.3259 | s/batch 0.29\n",
      "2021-07-20 11:36:41,989 INFO: | epoch   1 | step 4100 | batch 4100/11250 | lr 0.00006 0.00003 | loss 0.1795 | s/batch 0.27\n",
      "2021-07-20 11:36:55,014 INFO: | epoch   1 | step 4150 | batch 4150/11250 | lr 0.00006 0.00003 | loss 0.2272 | s/batch 0.26\n",
      "2021-07-20 11:37:10,296 INFO: | epoch   1 | step 4200 | batch 4200/11250 | lr 0.00006 0.00003 | loss 0.2465 | s/batch 0.31\n",
      "2021-07-20 11:37:23,717 INFO: | epoch   1 | step 4250 | batch 4250/11250 | lr 0.00006 0.00003 | loss 0.2326 | s/batch 0.27\n",
      "2021-07-20 11:37:37,986 INFO: | epoch   1 | step 4300 | batch 4300/11250 | lr 0.00006 0.00003 | loss 0.2730 | s/batch 0.29\n",
      "2021-07-20 11:37:52,382 INFO: | epoch   1 | step 4350 | batch 4350/11250 | lr 0.00006 0.00003 | loss 0.2211 | s/batch 0.29\n",
      "2021-07-20 11:38:07,312 INFO: | epoch   1 | step 4400 | batch 4400/11250 | lr 0.00006 0.00003 | loss 0.2357 | s/batch 0.30\n",
      "2021-07-20 11:38:21,362 INFO: | epoch   1 | step 4450 | batch 4450/11250 | lr 0.00006 0.00003 | loss 0.2294 | s/batch 0.28\n",
      "2021-07-20 11:38:37,343 INFO: | epoch   1 | step 4500 | batch 4500/11250 | lr 0.00006 0.00003 | loss 0.2662 | s/batch 0.32\n",
      "2021-07-20 11:38:52,623 INFO: | epoch   1 | step 4550 | batch 4550/11250 | lr 0.00006 0.00003 | loss 0.2757 | s/batch 0.31\n",
      "2021-07-20 11:39:03,053 INFO: | epoch   1 | step 4600 | batch 4600/11250 | lr 0.00006 0.00003 | loss 0.1944 | s/batch 0.21\n",
      "2021-07-20 11:39:16,891 INFO: | epoch   1 | step 4650 | batch 4650/11250 | lr 0.00006 0.00003 | loss 0.1617 | s/batch 0.28\n",
      "2021-07-20 11:39:30,390 INFO: | epoch   1 | step 4700 | batch 4700/11250 | lr 0.00006 0.00003 | loss 0.2064 | s/batch 0.27\n",
      "2021-07-20 11:39:45,299 INFO: | epoch   1 | step 4750 | batch 4750/11250 | lr 0.00006 0.00003 | loss 0.2345 | s/batch 0.30\n",
      "2021-07-20 11:39:59,656 INFO: | epoch   1 | step 4800 | batch 4800/11250 | lr 0.00006 0.00003 | loss 0.2206 | s/batch 0.29\n",
      "2021-07-20 11:40:15,649 INFO: | epoch   1 | step 4850 | batch 4850/11250 | lr 0.00006 0.00003 | loss 0.2894 | s/batch 0.32\n",
      "2021-07-20 11:40:29,848 INFO: | epoch   1 | step 4900 | batch 4900/11250 | lr 0.00006 0.00003 | loss 0.1920 | s/batch 0.28\n",
      "2021-07-20 11:40:44,253 INFO: | epoch   1 | step 4950 | batch 4950/11250 | lr 0.00006 0.00003 | loss 0.2490 | s/batch 0.29\n",
      "2021-07-20 11:40:57,108 INFO: | epoch   1 | step 5000 | batch 5000/11250 | lr 0.00005 0.00003 | loss 0.2096 | s/batch 0.26\n",
      "2021-07-20 11:41:12,367 INFO: | epoch   1 | step 5050 | batch 5050/11250 | lr 0.00005 0.00003 | loss 0.1851 | s/batch 0.31\n",
      "2021-07-20 11:41:26,809 INFO: | epoch   1 | step 5100 | batch 5100/11250 | lr 0.00005 0.00003 | loss 0.1991 | s/batch 0.29\n",
      "2021-07-20 11:41:40,621 INFO: | epoch   1 | step 5150 | batch 5150/11250 | lr 0.00005 0.00003 | loss 0.2219 | s/batch 0.28\n",
      "2021-07-20 11:41:54,606 INFO: | epoch   1 | step 5200 | batch 5200/11250 | lr 0.00005 0.00003 | loss 0.2263 | s/batch 0.28\n",
      "2021-07-20 11:42:08,963 INFO: | epoch   1 | step 5250 | batch 5250/11250 | lr 0.00005 0.00003 | loss 0.2448 | s/batch 0.29\n",
      "2021-07-20 11:42:21,925 INFO: | epoch   1 | step 5300 | batch 5300/11250 | lr 0.00005 0.00003 | loss 0.1651 | s/batch 0.26\n",
      "2021-07-20 11:42:36,660 INFO: | epoch   1 | step 5350 | batch 5350/11250 | lr 0.00005 0.00003 | loss 0.2046 | s/batch 0.29\n",
      "2021-07-20 11:42:49,972 INFO: | epoch   1 | step 5400 | batch 5400/11250 | lr 0.00005 0.00003 | loss 0.1743 | s/batch 0.27\n",
      "2021-07-20 11:43:05,616 INFO: | epoch   1 | step 5450 | batch 5450/11250 | lr 0.00005 0.00003 | loss 0.2334 | s/batch 0.31\n",
      "2021-07-20 11:43:20,021 INFO: | epoch   1 | step 5500 | batch 5500/11250 | lr 0.00005 0.00003 | loss 0.2216 | s/batch 0.29\n",
      "2021-07-20 11:43:37,418 INFO: | epoch   1 | step 5550 | batch 5550/11250 | lr 0.00005 0.00003 | loss 0.2528 | s/batch 0.35\n",
      "2021-07-20 11:43:52,407 INFO: | epoch   1 | step 5600 | batch 5600/11250 | lr 0.00005 0.00003 | loss 0.2639 | s/batch 0.30\n",
      "2021-07-20 11:44:05,253 INFO: | epoch   1 | step 5650 | batch 5650/11250 | lr 0.00005 0.00002 | loss 0.2043 | s/batch 0.26\n",
      "2021-07-20 11:44:20,536 INFO: | epoch   1 | step 5700 | batch 5700/11250 | lr 0.00005 0.00002 | loss 0.2179 | s/batch 0.31\n",
      "2021-07-20 11:44:34,557 INFO: | epoch   1 | step 5750 | batch 5750/11250 | lr 0.00005 0.00002 | loss 0.1669 | s/batch 0.28\n",
      "2021-07-20 11:44:49,580 INFO: | epoch   1 | step 5800 | batch 5800/11250 | lr 0.00005 0.00002 | loss 0.2089 | s/batch 0.30\n",
      "2021-07-20 11:45:05,726 INFO: | epoch   1 | step 5850 | batch 5850/11250 | lr 0.00005 0.00002 | loss 0.2387 | s/batch 0.32\n",
      "2021-07-20 11:45:19,190 INFO: | epoch   1 | step 5900 | batch 5900/11250 | lr 0.00005 0.00002 | loss 0.2251 | s/batch 0.27\n",
      "2021-07-20 11:45:33,099 INFO: | epoch   1 | step 5950 | batch 5950/11250 | lr 0.00005 0.00002 | loss 0.2626 | s/batch 0.28\n",
      "2021-07-20 11:45:47,873 INFO: | epoch   1 | step 6000 | batch 6000/11250 | lr 0.00004 0.00002 | loss 0.2030 | s/batch 0.30\n",
      "2021-07-20 11:46:04,055 INFO: | epoch   1 | step 6050 | batch 6050/11250 | lr 0.00004 0.00002 | loss 0.1963 | s/batch 0.32\n",
      "2021-07-20 11:46:18,931 INFO: | epoch   1 | step 6100 | batch 6100/11250 | lr 0.00004 0.00002 | loss 0.1812 | s/batch 0.30\n",
      "2021-07-20 11:46:32,994 INFO: | epoch   1 | step 6150 | batch 6150/11250 | lr 0.00004 0.00002 | loss 0.2622 | s/batch 0.28\n",
      "2021-07-20 11:46:49,104 INFO: | epoch   1 | step 6200 | batch 6200/11250 | lr 0.00004 0.00002 | loss 0.2198 | s/batch 0.32\n",
      "2021-07-20 11:47:04,620 INFO: | epoch   1 | step 6250 | batch 6250/11250 | lr 0.00004 0.00002 | loss 0.2637 | s/batch 0.31\n",
      "2021-07-20 11:47:19,230 INFO: | epoch   1 | step 6300 | batch 6300/11250 | lr 0.00004 0.00002 | loss 0.1761 | s/batch 0.29\n",
      "2021-07-20 11:47:33,413 INFO: | epoch   1 | step 6350 | batch 6350/11250 | lr 0.00004 0.00002 | loss 0.2292 | s/batch 0.28\n",
      "2021-07-20 11:47:47,103 INFO: | epoch   1 | step 6400 | batch 6400/11250 | lr 0.00004 0.00002 | loss 0.2454 | s/batch 0.27\n",
      "2021-07-20 11:48:01,245 INFO: | epoch   1 | step 6450 | batch 6450/11250 | lr 0.00004 0.00002 | loss 0.2100 | s/batch 0.28\n",
      "2021-07-20 11:48:18,012 INFO: | epoch   1 | step 6500 | batch 6500/11250 | lr 0.00004 0.00002 | loss 0.2355 | s/batch 0.34\n",
      "2021-07-20 11:48:32,992 INFO: | epoch   1 | step 6550 | batch 6550/11250 | lr 0.00004 0.00002 | loss 0.1914 | s/batch 0.30\n",
      "2021-07-20 11:48:48,946 INFO: | epoch   1 | step 6600 | batch 6600/11250 | lr 0.00004 0.00002 | loss 0.1704 | s/batch 0.32\n",
      "2021-07-20 11:49:01,836 INFO: | epoch   1 | step 6650 | batch 6650/11250 | lr 0.00004 0.00002 | loss 0.2120 | s/batch 0.26\n",
      "2021-07-20 11:49:17,664 INFO: | epoch   1 | step 6700 | batch 6700/11250 | lr 0.00004 0.00002 | loss 0.1720 | s/batch 0.32\n",
      "2021-07-20 11:49:32,946 INFO: | epoch   1 | step 6750 | batch 6750/11250 | lr 0.00004 0.00002 | loss 0.2387 | s/batch 0.31\n",
      "2021-07-20 11:49:48,156 INFO: | epoch   1 | step 6800 | batch 6800/11250 | lr 0.00004 0.00002 | loss 0.2067 | s/batch 0.30\n",
      "2021-07-20 11:50:02,167 INFO: | epoch   1 | step 6850 | batch 6850/11250 | lr 0.00004 0.00002 | loss 0.1960 | s/batch 0.28\n",
      "2021-07-20 11:50:16,287 INFO: | epoch   1 | step 6900 | batch 6900/11250 | lr 0.00004 0.00002 | loss 0.1583 | s/batch 0.28\n",
      "2021-07-20 11:50:31,593 INFO: | epoch   1 | step 6950 | batch 6950/11250 | lr 0.00004 0.00002 | loss 0.1820 | s/batch 0.31\n",
      "2021-07-20 11:50:45,919 INFO: | epoch   1 | step 7000 | batch 7000/11250 | lr 0.00003 0.00002 | loss 0.2016 | s/batch 0.29\n",
      "2021-07-20 11:51:00,232 INFO: | epoch   1 | step 7050 | batch 7050/11250 | lr 0.00003 0.00002 | loss 0.2250 | s/batch 0.29\n",
      "2021-07-20 11:51:16,652 INFO: | epoch   1 | step 7100 | batch 7100/11250 | lr 0.00003 0.00002 | loss 0.2268 | s/batch 0.33\n",
      "2021-07-20 11:51:31,981 INFO: | epoch   1 | step 7150 | batch 7150/11250 | lr 0.00003 0.00002 | loss 0.2018 | s/batch 0.31\n",
      "2021-07-20 11:51:47,068 INFO: | epoch   1 | step 7200 | batch 7200/11250 | lr 0.00003 0.00002 | loss 0.1376 | s/batch 0.30\n",
      "2021-07-20 11:52:00,356 INFO: | epoch   1 | step 7250 | batch 7250/11250 | lr 0.00003 0.00002 | loss 0.1951 | s/batch 0.27\n",
      "2021-07-20 11:52:16,495 INFO: | epoch   1 | step 7300 | batch 7300/11250 | lr 0.00003 0.00002 | loss 0.1964 | s/batch 0.32\n",
      "2021-07-20 11:52:32,163 INFO: | epoch   1 | step 7350 | batch 7350/11250 | lr 0.00003 0.00002 | loss 0.1722 | s/batch 0.31\n",
      "2021-07-20 11:52:46,411 INFO: | epoch   1 | step 7400 | batch 7400/11250 | lr 0.00003 0.00002 | loss 0.1847 | s/batch 0.28\n",
      "2021-07-20 11:53:01,646 INFO: | epoch   1 | step 7450 | batch 7450/11250 | lr 0.00003 0.00002 | loss 0.2076 | s/batch 0.30\n",
      "2021-07-20 11:53:17,218 INFO: | epoch   1 | step 7500 | batch 7500/11250 | lr 0.00003 0.00002 | loss 0.2086 | s/batch 0.31\n",
      "2021-07-20 11:53:31,459 INFO: | epoch   1 | step 7550 | batch 7550/11250 | lr 0.00003 0.00002 | loss 0.1803 | s/batch 0.28\n",
      "2021-07-20 11:53:45,613 INFO: | epoch   1 | step 7600 | batch 7600/11250 | lr 0.00003 0.00002 | loss 0.1329 | s/batch 0.28\n",
      "2021-07-20 11:54:01,172 INFO: | epoch   1 | step 7650 | batch 7650/11250 | lr 0.00003 0.00002 | loss 0.2109 | s/batch 0.31\n",
      "2021-07-20 11:54:14,443 INFO: | epoch   1 | step 7700 | batch 7700/11250 | lr 0.00003 0.00002 | loss 0.1771 | s/batch 0.27\n",
      "2021-07-20 11:54:29,040 INFO: | epoch   1 | step 7750 | batch 7750/11250 | lr 0.00003 0.00002 | loss 0.2159 | s/batch 0.29\n",
      "2021-07-20 11:54:42,599 INFO: | epoch   1 | step 7800 | batch 7800/11250 | lr 0.00003 0.00002 | loss 0.2523 | s/batch 0.27\n",
      "2021-07-20 11:54:56,799 INFO: | epoch   1 | step 7850 | batch 7850/11250 | lr 0.00003 0.00002 | loss 0.1969 | s/batch 0.28\n",
      "2021-07-20 11:55:10,210 INFO: | epoch   1 | step 7900 | batch 7900/11250 | lr 0.00003 0.00001 | loss 0.1645 | s/batch 0.27\n",
      "2021-07-20 11:55:25,640 INFO: | epoch   1 | step 7950 | batch 7950/11250 | lr 0.00003 0.00001 | loss 0.1862 | s/batch 0.31\n",
      "2021-07-20 11:55:40,235 INFO: | epoch   1 | step 8000 | batch 8000/11250 | lr 0.00002 0.00001 | loss 0.1808 | s/batch 0.29\n",
      "2021-07-20 11:55:54,215 INFO: | epoch   1 | step 8050 | batch 8050/11250 | lr 0.00002 0.00001 | loss 0.1823 | s/batch 0.28\n",
      "2021-07-20 11:56:09,181 INFO: | epoch   1 | step 8100 | batch 8100/11250 | lr 0.00002 0.00001 | loss 0.1850 | s/batch 0.30\n",
      "2021-07-20 11:56:25,297 INFO: | epoch   1 | step 8150 | batch 8150/11250 | lr 0.00002 0.00001 | loss 0.2073 | s/batch 0.32\n",
      "2021-07-20 11:56:38,292 INFO: | epoch   1 | step 8200 | batch 8200/11250 | lr 0.00002 0.00001 | loss 0.1953 | s/batch 0.26\n",
      "2021-07-20 11:56:51,991 INFO: | epoch   1 | step 8250 | batch 8250/11250 | lr 0.00002 0.00001 | loss 0.1688 | s/batch 0.27\n",
      "2021-07-20 11:57:05,441 INFO: | epoch   1 | step 8300 | batch 8300/11250 | lr 0.00002 0.00001 | loss 0.1979 | s/batch 0.27\n",
      "2021-07-20 11:57:22,064 INFO: | epoch   1 | step 8350 | batch 8350/11250 | lr 0.00002 0.00001 | loss 0.1509 | s/batch 0.33\n",
      "2021-07-20 11:57:37,475 INFO: | epoch   1 | step 8400 | batch 8400/11250 | lr 0.00002 0.00001 | loss 0.2484 | s/batch 0.31\n",
      "2021-07-20 11:57:52,589 INFO: | epoch   1 | step 8450 | batch 8450/11250 | lr 0.00002 0.00001 | loss 0.1677 | s/batch 0.30\n",
      "2021-07-20 11:58:07,115 INFO: | epoch   1 | step 8500 | batch 8500/11250 | lr 0.00002 0.00001 | loss 0.2045 | s/batch 0.29\n",
      "2021-07-20 11:58:21,539 INFO: | epoch   1 | step 8550 | batch 8550/11250 | lr 0.00002 0.00001 | loss 0.1803 | s/batch 0.29\n",
      "2021-07-20 11:58:35,524 INFO: | epoch   1 | step 8600 | batch 8600/11250 | lr 0.00002 0.00001 | loss 0.1314 | s/batch 0.28\n",
      "2021-07-20 11:58:51,911 INFO: | epoch   1 | step 8650 | batch 8650/11250 | lr 0.00002 0.00001 | loss 0.1875 | s/batch 0.33\n",
      "2021-07-20 11:59:05,673 INFO: | epoch   1 | step 8700 | batch 8700/11250 | lr 0.00002 0.00001 | loss 0.1962 | s/batch 0.28\n",
      "2021-07-20 11:59:19,497 INFO: | epoch   1 | step 8750 | batch 8750/11250 | lr 0.00002 0.00001 | loss 0.1881 | s/batch 0.28\n",
      "2021-07-20 11:59:35,907 INFO: | epoch   1 | step 8800 | batch 8800/11250 | lr 0.00002 0.00001 | loss 0.1936 | s/batch 0.33\n",
      "2021-07-20 11:59:48,273 INFO: | epoch   1 | step 8850 | batch 8850/11250 | lr 0.00002 0.00001 | loss 0.1581 | s/batch 0.25\n",
      "2021-07-20 12:00:02,280 INFO: | epoch   1 | step 8900 | batch 8900/11250 | lr 0.00002 0.00001 | loss 0.2112 | s/batch 0.28\n",
      "2021-07-20 12:00:16,565 INFO: | epoch   1 | step 8950 | batch 8950/11250 | lr 0.00002 0.00001 | loss 0.2273 | s/batch 0.29\n",
      "2021-07-20 12:00:30,397 INFO: | epoch   1 | step 9000 | batch 9000/11250 | lr 0.00002 0.00001 | loss 0.1987 | s/batch 0.28\n",
      "2021-07-20 12:00:45,126 INFO: | epoch   1 | step 9050 | batch 9050/11250 | lr 0.00002 0.00001 | loss 0.1611 | s/batch 0.29\n",
      "2021-07-20 12:00:59,936 INFO: | epoch   1 | step 9100 | batch 9100/11250 | lr 0.00002 0.00001 | loss 0.1587 | s/batch 0.30\n",
      "2021-07-20 12:01:13,763 INFO: | epoch   1 | step 9150 | batch 9150/11250 | lr 0.00002 0.00001 | loss 0.1892 | s/batch 0.28\n",
      "2021-07-20 12:01:26,816 INFO: | epoch   1 | step 9200 | batch 9200/11250 | lr 0.00002 0.00001 | loss 0.2116 | s/batch 0.26\n",
      "2021-07-20 12:01:38,887 INFO: | epoch   1 | step 9250 | batch 9250/11250 | lr 0.00002 0.00001 | loss 0.1412 | s/batch 0.24\n",
      "2021-07-20 12:01:52,731 INFO: | epoch   1 | step 9300 | batch 9300/11250 | lr 0.00002 0.00001 | loss 0.1368 | s/batch 0.28\n",
      "2021-07-20 12:02:06,861 INFO: | epoch   1 | step 9350 | batch 9350/11250 | lr 0.00002 0.00001 | loss 0.1562 | s/batch 0.28\n",
      "2021-07-20 12:02:18,677 INFO: | epoch   1 | step 9400 | batch 9400/11250 | lr 0.00002 0.00001 | loss 0.1797 | s/batch 0.24\n",
      "2021-07-20 12:02:31,980 INFO: | epoch   1 | step 9450 | batch 9450/11250 | lr 0.00002 0.00001 | loss 0.1745 | s/batch 0.27\n",
      "2021-07-20 12:02:45,154 INFO: | epoch   1 | step 9500 | batch 9500/11250 | lr 0.00002 0.00001 | loss 0.1967 | s/batch 0.26\n",
      "2021-07-20 12:03:00,750 INFO: | epoch   1 | step 9550 | batch 9550/11250 | lr 0.00002 0.00001 | loss 0.1368 | s/batch 0.31\n",
      "2021-07-20 12:03:13,245 INFO: | epoch   1 | step 9600 | batch 9600/11250 | lr 0.00002 0.00001 | loss 0.2170 | s/batch 0.25\n",
      "2021-07-20 12:03:26,748 INFO: | epoch   1 | step 9650 | batch 9650/11250 | lr 0.00002 0.00001 | loss 0.2023 | s/batch 0.27\n",
      "2021-07-20 12:03:41,297 INFO: | epoch   1 | step 9700 | batch 9700/11250 | lr 0.00002 0.00001 | loss 0.1880 | s/batch 0.29\n",
      "2021-07-20 12:03:56,431 INFO: | epoch   1 | step 9750 | batch 9750/11250 | lr 0.00002 0.00001 | loss 0.1615 | s/batch 0.30\n",
      "2021-07-20 12:04:12,671 INFO: | epoch   1 | step 9800 | batch 9800/11250 | lr 0.00002 0.00001 | loss 0.1736 | s/batch 0.32\n",
      "2021-07-20 12:04:27,194 INFO: | epoch   1 | step 9850 | batch 9850/11250 | lr 0.00002 0.00001 | loss 0.2148 | s/batch 0.29\n",
      "2021-07-20 12:04:39,382 INFO: | epoch   1 | step 9900 | batch 9900/11250 | lr 0.00002 0.00001 | loss 0.1344 | s/batch 0.24\n",
      "2021-07-20 12:04:55,311 INFO: | epoch   1 | step 9950 | batch 9950/11250 | lr 0.00002 0.00001 | loss 0.1430 | s/batch 0.32\n",
      "2021-07-20 12:05:09,769 INFO: | epoch   1 | step 10000 | batch 10000/11250 | lr 0.00001 0.00001 | loss 0.2187 | s/batch 0.29\n",
      "2021-07-20 12:05:26,223 INFO: | epoch   1 | step 10050 | batch 10050/11250 | lr 0.00001 0.00001 | loss 0.1629 | s/batch 0.33\n",
      "2021-07-20 12:05:39,389 INFO: | epoch   1 | step 10100 | batch 10100/11250 | lr 0.00001 0.00001 | loss 0.1941 | s/batch 0.26\n",
      "2021-07-20 12:05:53,639 INFO: | epoch   1 | step 10150 | batch 10150/11250 | lr 0.00001 0.00000 | loss 0.1420 | s/batch 0.28\n",
      "2021-07-20 12:06:08,071 INFO: | epoch   1 | step 10200 | batch 10200/11250 | lr 0.00001 0.00000 | loss 0.1761 | s/batch 0.29\n",
      "2021-07-20 12:06:22,264 INFO: | epoch   1 | step 10250 | batch 10250/11250 | lr 0.00001 0.00000 | loss 0.1485 | s/batch 0.28\n",
      "2021-07-20 12:06:37,293 INFO: | epoch   1 | step 10300 | batch 10300/11250 | lr 0.00001 0.00000 | loss 0.1870 | s/batch 0.30\n",
      "2021-07-20 12:06:51,501 INFO: | epoch   1 | step 10350 | batch 10350/11250 | lr 0.00001 0.00000 | loss 0.1990 | s/batch 0.28\n",
      "2021-07-20 12:07:06,191 INFO: | epoch   1 | step 10400 | batch 10400/11250 | lr 0.00001 0.00000 | loss 0.1835 | s/batch 0.29\n",
      "2021-07-20 12:07:20,813 INFO: | epoch   1 | step 10450 | batch 10450/11250 | lr 0.00001 0.00000 | loss 0.1917 | s/batch 0.29\n",
      "2021-07-20 12:07:36,319 INFO: | epoch   1 | step 10500 | batch 10500/11250 | lr 0.00001 0.00000 | loss 0.1353 | s/batch 0.31\n",
      "2021-07-20 12:07:49,826 INFO: | epoch   1 | step 10550 | batch 10550/11250 | lr 0.00001 0.00000 | loss 0.1664 | s/batch 0.27\n",
      "2021-07-20 12:08:03,987 INFO: | epoch   1 | step 10600 | batch 10600/11250 | lr 0.00001 0.00000 | loss 0.2141 | s/batch 0.28\n",
      "2021-07-20 12:08:15,945 INFO: | epoch   1 | step 10650 | batch 10650/11250 | lr 0.00001 0.00000 | loss 0.1443 | s/batch 0.24\n",
      "2021-07-20 12:08:28,963 INFO: | epoch   1 | step 10700 | batch 10700/11250 | lr 0.00001 0.00000 | loss 0.1702 | s/batch 0.26\n",
      "2021-07-20 12:08:42,955 INFO: | epoch   1 | step 10750 | batch 10750/11250 | lr 0.00001 0.00000 | loss 0.1700 | s/batch 0.28\n",
      "2021-07-20 12:08:55,999 INFO: | epoch   1 | step 10800 | batch 10800/11250 | lr 0.00001 0.00000 | loss 0.1637 | s/batch 0.26\n",
      "2021-07-20 12:09:08,771 INFO: | epoch   1 | step 10850 | batch 10850/11250 | lr 0.00001 0.00000 | loss 0.2180 | s/batch 0.26\n",
      "2021-07-20 12:09:22,573 INFO: | epoch   1 | step 10900 | batch 10900/11250 | lr 0.00001 0.00000 | loss 0.1820 | s/batch 0.28\n",
      "2021-07-20 12:09:35,918 INFO: | epoch   1 | step 10950 | batch 10950/11250 | lr 0.00001 0.00000 | loss 0.1684 | s/batch 0.27\n",
      "2021-07-20 12:09:49,850 INFO: | epoch   1 | step 11000 | batch 11000/11250 | lr 0.00001 0.00000 | loss 0.1498 | s/batch 0.28\n",
      "2021-07-20 12:10:03,847 INFO: | epoch   1 | step 11050 | batch 11050/11250 | lr 0.00001 0.00000 | loss 0.1296 | s/batch 0.28\n",
      "2021-07-20 12:10:18,571 INFO: | epoch   1 | step 11100 | batch 11100/11250 | lr 0.00001 0.00000 | loss 0.1494 | s/batch 0.29\n",
      "2021-07-20 12:10:32,371 INFO: | epoch   1 | step 11150 | batch 11150/11250 | lr 0.00001 0.00000 | loss 0.1458 | s/batch 0.28\n",
      "2021-07-20 12:10:46,611 INFO: | epoch   1 | step 11200 | batch 11200/11250 | lr 0.00001 0.00000 | loss 0.1698 | s/batch 0.28\n",
      "2021-07-20 12:11:00,487 INFO: | epoch   1 | step 11250 | batch 11250/11250 | lr 0.00001 0.00000 | loss 0.1620 | s/batch 0.28\n",
      "2021-07-20 12:11:00,732 INFO: | epoch   1 | score (90.91, 89.14, 89.99) | f1 89.99 | loss 0.2542 | time 3210.83\n",
      "2021-07-20 12:11:01,111 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9216    0.9322    0.9269     35027\n",
      "          股票     0.9258    0.9370    0.9314     33251\n",
      "          体育     0.9790    0.9814    0.9802     28283\n",
      "          娱乐     0.9389    0.9524    0.9456     19920\n",
      "          时政     0.8832    0.8959    0.8895     13515\n",
      "          社会     0.8698    0.8619    0.8659     11009\n",
      "          教育     0.9304    0.9232    0.9268      8987\n",
      "          财经     0.8718    0.8076    0.8385      7957\n",
      "          家居     0.8901    0.8890    0.8896      7063\n",
      "          游戏     0.9050    0.8771    0.8909      5291\n",
      "          房产     0.9217    0.8911    0.9062      4428\n",
      "          时尚     0.8879    0.8517    0.8694      2818\n",
      "          彩票     0.9231    0.8598    0.8903      1633\n",
      "          星座     0.8793    0.8191    0.8481       818\n",
      "\n",
      "    accuracy                         0.9233    180000\n",
      "   macro avg     0.9091    0.8914    0.8999    180000\n",
      "weighted avg     0.9231    0.9233    0.9231    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set cuda\n",
    "gpu = 0\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bert_dataset_loader import get_examples, data_iter\n",
    "from bert_data import dev_data, test_data, train_data\n",
    "from bert_optimizer import Optimizer\n",
    "from bert_score import get_score, reformat\n",
    "import pandas as pd\n",
    "from bert_vocab import Vocab\n",
    "vocab = Vocab(train_data)\n",
    "from bert_module import Model\n",
    "model = Model(vocab)\n",
    "\n",
    "\n",
    "# build trainer\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clip = 5.0\n",
    "epochs = 1\n",
    "early_stops = 3\n",
    "log_interval = 50\n",
    "\n",
    "test_batch_size = 16\n",
    "train_batch_size = 16\n",
    "\n",
    "save_model = '/home/featurize/data/bert.bin'\n",
    "save_test = '/home/featurize/data/bert.csv'\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.report = True\n",
    "\n",
    "        self.train_data = get_examples(train_data, model.word_encoder, vocab)\n",
    "        self.dev_data = get_examples(dev_data, model.word_encoder, vocab)\n",
    "        self.test_data = get_examples(test_data, model.word_encoder, vocab)\n",
    "        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n",
    "\n",
    "        # criterion\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # label name\n",
    "        self.target_names = vocab.target_names\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)\n",
    "\n",
    "        # count\n",
    "        self.step = 0\n",
    "        self.early_stop = -1\n",
    "        self.best_train_f1, self.best_dev_f1 = 0, 0\n",
    "        self.last_epoch = epochs\n",
    "\n",
    "    def train(self):\n",
    "        logging.info('Start training...')\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_f1 = self._train(epoch)\n",
    "\n",
    "            dev_f1 = self._eval(epoch)\n",
    "\n",
    "            if self.best_dev_f1 <= dev_f1:\n",
    "                logging.info(\n",
    "                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n",
    "                torch.save(self.model.state_dict(), save_model)\n",
    "\n",
    "                self.best_train_f1 = train_f1\n",
    "                self.best_dev_f1 = dev_f1\n",
    "                self.early_stop = 0\n",
    "            else:\n",
    "                self.early_stop += 1\n",
    "                if self.early_stop == early_stops:\n",
    "                    logging.info(\n",
    "                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n",
    "                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n",
    "                    self.last_epoch = epoch\n",
    "                    break\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(torch.load(save_model))\n",
    "        self._eval(self.last_epoch + 1, test=True)\n",
    "\n",
    "    def _train(self, epoch):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        epoch_start_time = time.time()\n",
    "        overall_losses = 0\n",
    "        losses = 0\n",
    "        batch_idx = 1\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "\n",
    "            batch_outputs = self.model(batch_inputs)   # b X num_labels\n",
    "            loss = self.criterion(batch_outputs, batch_labels)  # softmax + 交叉熵\n",
    "            loss.backward()\n",
    "\n",
    "            loss_value = loss.detach().cpu().item()\n",
    "            losses += loss_value\n",
    "            overall_losses += loss_value\n",
    "\n",
    "            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "            y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n",
    "            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                lrs = self.optimizer.get_lr()\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n",
    "                        epoch, self.step, batch_idx, self.batch_num, lrs,\n",
    "                        losses / log_interval,\n",
    "                        elapsed / log_interval))\n",
    "\n",
    "                losses = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "        overall_losses /= self.batch_num\n",
    "        during_time = time.time() - epoch_start_time\n",
    "\n",
    "        # reformat\n",
    "        overall_losses = reformat(overall_losses, 4)\n",
    "        score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "        logging.info(\n",
    "            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                                  overall_losses,\n",
    "                                                                                  during_time))\n",
    "        if set(y_true) == set(y_pred) and self.report:\n",
    "            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "            logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def _eval(self, epoch, test=False):\n",
    "        self.model.eval()\n",
    "        start_time = time.time()\n",
    "        data = self.test_data if test else self.dev_data\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "\n",
    "                batch_outputs = self.model(batch_inputs)\n",
    "                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "                y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "            during_time = time.time() - start_time\n",
    "\n",
    "            if test:\n",
    "                df = pd.DataFrame({'label': y_pred})\n",
    "                df.to_csv(save_test, index=False, sep=',')\n",
    "            else:\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                                  during_time))\n",
    "                if set(y_true) == set(y_pred) and self.report:\n",
    "                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "                    logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def batch2tensor(self, batch_data):\n",
    "        '''\n",
    "            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n",
    "        '''\n",
    "        batch_size = len(batch_data)\n",
    "        doc_labels = []\n",
    "        doc_lens = []\n",
    "        doc_max_sent_len = []\n",
    "        for doc_data in batch_data:\n",
    "            doc_labels.append(doc_data[0])\n",
    "            doc_lens.append(doc_data[1])\n",
    "            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n",
    "            max_sent_len = max(sent_lens)\n",
    "            doc_max_sent_len.append(max_sent_len)\n",
    "\n",
    "        max_doc_len = max(doc_lens)\n",
    "        max_sent_len = max(doc_max_sent_len)\n",
    "\n",
    "        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n",
    "        batch_labels = torch.LongTensor(doc_labels)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for sent_idx in range(doc_lens[b]):\n",
    "                sent_data = batch_data[b][2][sent_idx]\n",
    "                for word_idx in range(sent_data[0]):\n",
    "                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n",
    "                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n",
    "                    batch_masks[b, sent_idx, word_idx] = 1\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_inputs1 = batch_inputs1.to(device)\n",
    "            batch_inputs2 = batch_inputs2.to(device)\n",
    "            batch_masks = batch_masks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels\n",
    "\n",
    "\n",
    "# train\n",
    "trainer = Trainer(model, vocab)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-institution",
   "metadata": {},
   "source": [
    "# test\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-carol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
