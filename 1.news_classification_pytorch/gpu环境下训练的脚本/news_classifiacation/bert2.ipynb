{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "upset-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 09:21:03,326 INFO: Use cuda: True, gpu id: 0.\n",
      "2021-07-23 09:21:47,329 INFO: Build vocab: words 5980, labels 14.\n",
      "2021-07-23 09:21:47,504 INFO: Build Bert vocab with size 6874.\n",
      "Some weights of the model checkpoint at /home/featurize/data/bert_mini_2/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2021-07-23 09:21:47,561 INFO: Build Bert encoder with pooled False.\n",
      "2021-07-23 09:21:47,580 INFO: Use cuda: True, gpu id: 0.\n",
      "2021-07-23 09:21:50,535 INFO: Build model with bert word encoder, lstm sent encoder.\n",
      "2021-07-23 09:21:50,537 INFO: Model param num: 7.95 M.\n",
      "2021-07-23 09:38:14,090 INFO: Total 180000 docs.\n",
      "2021-07-23 09:40:03,222 INFO: Total 20000 docs.\n",
      "2021-07-23 09:44:37,437 INFO: Total 50000 docs.\n",
      "2021-07-23 09:44:37,441 INFO: Start training...\n",
      "/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "2021-07-23 09:44:51,135 INFO: | epoch   1 | step  50 | batch  50/11250 | lr 0.00020 0.00005 | loss 2.3845 | s/batch 0.27\n",
      "2021-07-23 09:45:03,248 INFO: | epoch   1 | step 100 | batch 100/11250 | lr 0.00020 0.00005 | loss 2.2768 | s/batch 0.24\n",
      "2021-07-23 09:45:14,498 INFO: | epoch   1 | step 150 | batch 150/11250 | lr 0.00020 0.00005 | loss 2.2558 | s/batch 0.22\n",
      "2021-07-23 09:45:26,968 INFO: | epoch   1 | step 200 | batch 200/11250 | lr 0.00020 0.00005 | loss 2.2077 | s/batch 0.25\n",
      "2021-07-23 09:45:38,906 INFO: | epoch   1 | step 250 | batch 250/11250 | lr 0.00020 0.00005 | loss 2.2823 | s/batch 0.24\n",
      "2021-07-23 09:45:50,681 INFO: | epoch   1 | step 300 | batch 300/11250 | lr 0.00020 0.00005 | loss 2.2699 | s/batch 0.24\n",
      "2021-07-23 09:46:02,621 INFO: | epoch   1 | step 350 | batch 350/11250 | lr 0.00020 0.00005 | loss 2.2114 | s/batch 0.24\n",
      "2021-07-23 09:46:15,862 INFO: | epoch   1 | step 400 | batch 400/11250 | lr 0.00020 0.00005 | loss 1.7921 | s/batch 0.26\n",
      "2021-07-23 09:46:27,721 INFO: | epoch   1 | step 450 | batch 450/11250 | lr 0.00020 0.00005 | loss 1.6068 | s/batch 0.24\n",
      "2021-07-23 09:46:38,521 INFO: | epoch   1 | step 500 | batch 500/11250 | lr 0.00020 0.00005 | loss 1.5039 | s/batch 0.22\n",
      "2021-07-23 09:46:49,847 INFO: | epoch   1 | step 550 | batch 550/11250 | lr 0.00020 0.00005 | loss 1.4910 | s/batch 0.23\n",
      "2021-07-23 09:47:01,570 INFO: | epoch   1 | step 600 | batch 600/11250 | lr 0.00020 0.00005 | loss 1.3615 | s/batch 0.23\n",
      "2021-07-23 09:47:13,906 INFO: | epoch   1 | step 650 | batch 650/11250 | lr 0.00020 0.00005 | loss 1.4000 | s/batch 0.25\n",
      "2021-07-23 09:47:27,181 INFO: | epoch   1 | step 700 | batch 700/11250 | lr 0.00020 0.00005 | loss 1.4804 | s/batch 0.27\n",
      "2021-07-23 09:47:40,004 INFO: | epoch   1 | step 750 | batch 750/11250 | lr 0.00020 0.00005 | loss 1.4282 | s/batch 0.26\n",
      "2021-07-23 09:47:51,399 INFO: | epoch   1 | step 800 | batch 800/11250 | lr 0.00020 0.00005 | loss 1.4347 | s/batch 0.23\n",
      "2021-07-23 09:48:04,007 INFO: | epoch   1 | step 850 | batch 850/11250 | lr 0.00020 0.00005 | loss 1.3943 | s/batch 0.25\n",
      "2021-07-23 09:48:15,817 INFO: | epoch   1 | step 900 | batch 900/11250 | lr 0.00020 0.00005 | loss 1.3428 | s/batch 0.24\n",
      "2021-07-23 09:48:28,147 INFO: | epoch   1 | step 950 | batch 950/11250 | lr 0.00020 0.00005 | loss 1.3489 | s/batch 0.25\n",
      "2021-07-23 09:48:42,368 INFO: | epoch   1 | step 1000 | batch 1000/11250 | lr 0.00015 0.00005 | loss 1.3351 | s/batch 0.28\n",
      "2021-07-23 09:48:55,535 INFO: | epoch   1 | step 1050 | batch 1050/11250 | lr 0.00015 0.00005 | loss 1.2609 | s/batch 0.26\n",
      "2021-07-23 09:49:08,635 INFO: | epoch   1 | step 1100 | batch 1100/11250 | lr 0.00015 0.00005 | loss 1.2962 | s/batch 0.26\n",
      "2021-07-23 09:49:21,338 INFO: | epoch   1 | step 1150 | batch 1150/11250 | lr 0.00015 0.00004 | loss 1.2305 | s/batch 0.25\n",
      "2021-07-23 09:49:35,420 INFO: | epoch   1 | step 1200 | batch 1200/11250 | lr 0.00015 0.00004 | loss 1.2689 | s/batch 0.28\n",
      "2021-07-23 09:49:47,092 INFO: | epoch   1 | step 1250 | batch 1250/11250 | lr 0.00015 0.00004 | loss 1.2512 | s/batch 0.23\n",
      "2021-07-23 09:50:00,181 INFO: | epoch   1 | step 1300 | batch 1300/11250 | lr 0.00015 0.00004 | loss 1.2243 | s/batch 0.26\n",
      "2021-07-23 09:50:14,906 INFO: | epoch   1 | step 1350 | batch 1350/11250 | lr 0.00015 0.00004 | loss 1.2561 | s/batch 0.29\n",
      "2021-07-23 09:50:26,364 INFO: | epoch   1 | step 1400 | batch 1400/11250 | lr 0.00015 0.00004 | loss 1.1831 | s/batch 0.23\n",
      "2021-07-23 09:50:41,541 INFO: | epoch   1 | step 1450 | batch 1450/11250 | lr 0.00015 0.00004 | loss 1.2384 | s/batch 0.30\n",
      "2021-07-23 09:50:54,375 INFO: | epoch   1 | step 1500 | batch 1500/11250 | lr 0.00015 0.00004 | loss 1.1946 | s/batch 0.26\n",
      "2021-07-23 09:51:07,584 INFO: | epoch   1 | step 1550 | batch 1550/11250 | lr 0.00015 0.00004 | loss 1.2192 | s/batch 0.26\n",
      "2021-07-23 09:51:19,967 INFO: | epoch   1 | step 1600 | batch 1600/11250 | lr 0.00015 0.00004 | loss 1.1521 | s/batch 0.25\n",
      "2021-07-23 09:51:30,879 INFO: | epoch   1 | step 1650 | batch 1650/11250 | lr 0.00015 0.00004 | loss 1.1870 | s/batch 0.22\n",
      "2021-07-23 09:51:42,616 INFO: | epoch   1 | step 1700 | batch 1700/11250 | lr 0.00015 0.00004 | loss 1.1257 | s/batch 0.23\n",
      "2021-07-23 09:51:56,176 INFO: | epoch   1 | step 1750 | batch 1750/11250 | lr 0.00015 0.00004 | loss 1.0966 | s/batch 0.27\n",
      "2021-07-23 09:52:08,724 INFO: | epoch   1 | step 1800 | batch 1800/11250 | lr 0.00015 0.00004 | loss 1.0882 | s/batch 0.25\n",
      "2021-07-23 09:52:21,427 INFO: | epoch   1 | step 1850 | batch 1850/11250 | lr 0.00015 0.00004 | loss 1.0619 | s/batch 0.25\n",
      "2021-07-23 09:52:34,843 INFO: | epoch   1 | step 1900 | batch 1900/11250 | lr 0.00015 0.00004 | loss 1.0243 | s/batch 0.27\n",
      "2021-07-23 09:52:46,833 INFO: | epoch   1 | step 1950 | batch 1950/11250 | lr 0.00015 0.00004 | loss 1.1143 | s/batch 0.24\n",
      "2021-07-23 09:52:58,216 INFO: | epoch   1 | step 2000 | batch 2000/11250 | lr 0.00011 0.00004 | loss 0.9912 | s/batch 0.23\n",
      "2021-07-23 09:53:11,625 INFO: | epoch   1 | step 2050 | batch 2050/11250 | lr 0.00011 0.00004 | loss 1.1203 | s/batch 0.27\n",
      "2021-07-23 09:53:23,583 INFO: | epoch   1 | step 2100 | batch 2100/11250 | lr 0.00011 0.00004 | loss 1.0759 | s/batch 0.24\n",
      "2021-07-23 09:53:36,239 INFO: | epoch   1 | step 2150 | batch 2150/11250 | lr 0.00011 0.00004 | loss 1.0840 | s/batch 0.25\n",
      "2021-07-23 09:53:48,723 INFO: | epoch   1 | step 2200 | batch 2200/11250 | lr 0.00011 0.00004 | loss 0.9527 | s/batch 0.25\n",
      "2021-07-23 09:54:02,439 INFO: | epoch   1 | step 2250 | batch 2250/11250 | lr 0.00011 0.00004 | loss 1.0592 | s/batch 0.27\n",
      "2021-07-23 09:54:15,263 INFO: | epoch   1 | step 2300 | batch 2300/11250 | lr 0.00011 0.00004 | loss 0.9828 | s/batch 0.26\n",
      "2021-07-23 09:54:27,240 INFO: | epoch   1 | step 2350 | batch 2350/11250 | lr 0.00011 0.00004 | loss 1.0076 | s/batch 0.24\n",
      "2021-07-23 09:54:40,540 INFO: | epoch   1 | step 2400 | batch 2400/11250 | lr 0.00011 0.00004 | loss 0.9969 | s/batch 0.27\n",
      "2021-07-23 09:54:55,172 INFO: | epoch   1 | step 2450 | batch 2450/11250 | lr 0.00011 0.00004 | loss 0.8819 | s/batch 0.29\n",
      "2021-07-23 09:55:07,591 INFO: | epoch   1 | step 2500 | batch 2500/11250 | lr 0.00011 0.00004 | loss 0.9516 | s/batch 0.25\n",
      "2021-07-23 09:55:20,260 INFO: | epoch   1 | step 2550 | batch 2550/11250 | lr 0.00011 0.00004 | loss 0.8907 | s/batch 0.25\n",
      "2021-07-23 09:55:34,376 INFO: | epoch   1 | step 2600 | batch 2600/11250 | lr 0.00011 0.00004 | loss 0.9583 | s/batch 0.28\n",
      "2021-07-23 09:55:46,140 INFO: | epoch   1 | step 2650 | batch 2650/11250 | lr 0.00011 0.00004 | loss 0.8674 | s/batch 0.24\n",
      "2021-07-23 09:55:58,427 INFO: | epoch   1 | step 2700 | batch 2700/11250 | lr 0.00011 0.00004 | loss 0.8773 | s/batch 0.25\n",
      "2021-07-23 09:56:09,695 INFO: | epoch   1 | step 2750 | batch 2750/11250 | lr 0.00011 0.00004 | loss 0.8378 | s/batch 0.23\n",
      "2021-07-23 09:56:23,328 INFO: | epoch   1 | step 2800 | batch 2800/11250 | lr 0.00011 0.00004 | loss 0.8824 | s/batch 0.27\n",
      "2021-07-23 09:56:35,853 INFO: | epoch   1 | step 2850 | batch 2850/11250 | lr 0.00011 0.00004 | loss 0.7686 | s/batch 0.25\n",
      "2021-07-23 09:56:48,593 INFO: | epoch   1 | step 2900 | batch 2900/11250 | lr 0.00011 0.00004 | loss 0.8351 | s/batch 0.25\n",
      "2021-07-23 09:57:01,459 INFO: | epoch   1 | step 2950 | batch 2950/11250 | lr 0.00011 0.00004 | loss 0.7832 | s/batch 0.26\n",
      "2021-07-23 09:57:12,872 INFO: | epoch   1 | step 3000 | batch 3000/11250 | lr 0.00008 0.00004 | loss 0.7953 | s/batch 0.23\n",
      "2021-07-23 09:57:26,565 INFO: | epoch   1 | step 3050 | batch 3050/11250 | lr 0.00008 0.00004 | loss 0.6993 | s/batch 0.27\n",
      "2021-07-23 09:57:37,585 INFO: | epoch   1 | step 3100 | batch 3100/11250 | lr 0.00008 0.00004 | loss 0.6861 | s/batch 0.22\n",
      "2021-07-23 09:57:48,772 INFO: | epoch   1 | step 3150 | batch 3150/11250 | lr 0.00008 0.00004 | loss 0.7415 | s/batch 0.22\n",
      "2021-07-23 09:58:00,674 INFO: | epoch   1 | step 3200 | batch 3200/11250 | lr 0.00008 0.00004 | loss 0.6508 | s/batch 0.24\n",
      "2021-07-23 09:58:12,337 INFO: | epoch   1 | step 3250 | batch 3250/11250 | lr 0.00008 0.00004 | loss 0.7072 | s/batch 0.23\n",
      "2021-07-23 09:58:24,581 INFO: | epoch   1 | step 3300 | batch 3300/11250 | lr 0.00008 0.00004 | loss 0.7038 | s/batch 0.24\n",
      "2021-07-23 09:58:36,521 INFO: | epoch   1 | step 3350 | batch 3350/11250 | lr 0.00008 0.00004 | loss 0.6656 | s/batch 0.24\n",
      "2021-07-23 09:58:49,151 INFO: | epoch   1 | step 3400 | batch 3400/11250 | lr 0.00008 0.00003 | loss 0.6410 | s/batch 0.25\n",
      "2021-07-23 09:59:02,065 INFO: | epoch   1 | step 3450 | batch 3450/11250 | lr 0.00008 0.00003 | loss 0.6870 | s/batch 0.26\n",
      "2021-07-23 09:59:14,849 INFO: | epoch   1 | step 3500 | batch 3500/11250 | lr 0.00008 0.00003 | loss 0.6775 | s/batch 0.26\n",
      "2021-07-23 09:59:28,507 INFO: | epoch   1 | step 3550 | batch 3550/11250 | lr 0.00008 0.00003 | loss 0.6992 | s/batch 0.27\n",
      "2021-07-23 09:59:40,165 INFO: | epoch   1 | step 3600 | batch 3600/11250 | lr 0.00008 0.00003 | loss 0.6593 | s/batch 0.23\n",
      "2021-07-23 09:59:51,481 INFO: | epoch   1 | step 3650 | batch 3650/11250 | lr 0.00008 0.00003 | loss 0.6134 | s/batch 0.23\n",
      "2021-07-23 10:00:04,504 INFO: | epoch   1 | step 3700 | batch 3700/11250 | lr 0.00008 0.00003 | loss 0.5869 | s/batch 0.26\n",
      "2021-07-23 10:00:16,390 INFO: | epoch   1 | step 3750 | batch 3750/11250 | lr 0.00008 0.00003 | loss 0.6146 | s/batch 0.24\n",
      "2021-07-23 10:00:28,067 INFO: | epoch   1 | step 3800 | batch 3800/11250 | lr 0.00008 0.00003 | loss 0.6188 | s/batch 0.23\n",
      "2021-07-23 10:00:39,492 INFO: | epoch   1 | step 3850 | batch 3850/11250 | lr 0.00008 0.00003 | loss 0.7011 | s/batch 0.23\n",
      "2021-07-23 10:00:50,814 INFO: | epoch   1 | step 3900 | batch 3900/11250 | lr 0.00008 0.00003 | loss 0.5910 | s/batch 0.23\n",
      "2021-07-23 10:01:04,286 INFO: | epoch   1 | step 3950 | batch 3950/11250 | lr 0.00008 0.00003 | loss 0.6197 | s/batch 0.27\n",
      "2021-07-23 10:01:16,385 INFO: | epoch   1 | step 4000 | batch 4000/11250 | lr 0.00006 0.00003 | loss 0.6045 | s/batch 0.24\n",
      "2021-07-23 10:01:29,184 INFO: | epoch   1 | step 4050 | batch 4050/11250 | lr 0.00006 0.00003 | loss 0.6116 | s/batch 0.26\n",
      "2021-07-23 10:01:41,512 INFO: | epoch   1 | step 4100 | batch 4100/11250 | lr 0.00006 0.00003 | loss 0.5877 | s/batch 0.25\n",
      "2021-07-23 10:01:52,879 INFO: | epoch   1 | step 4150 | batch 4150/11250 | lr 0.00006 0.00003 | loss 0.5176 | s/batch 0.23\n",
      "2021-07-23 10:02:06,309 INFO: | epoch   1 | step 4200 | batch 4200/11250 | lr 0.00006 0.00003 | loss 0.5938 | s/batch 0.27\n",
      "2021-07-23 10:02:16,784 INFO: | epoch   1 | step 4250 | batch 4250/11250 | lr 0.00006 0.00003 | loss 0.6073 | s/batch 0.21\n",
      "2021-07-23 10:02:27,045 INFO: | epoch   1 | step 4300 | batch 4300/11250 | lr 0.00006 0.00003 | loss 0.5194 | s/batch 0.21\n",
      "2021-07-23 10:02:40,061 INFO: | epoch   1 | step 4350 | batch 4350/11250 | lr 0.00006 0.00003 | loss 0.5526 | s/batch 0.26\n",
      "2021-07-23 10:02:52,556 INFO: | epoch   1 | step 4400 | batch 4400/11250 | lr 0.00006 0.00003 | loss 0.4899 | s/batch 0.25\n",
      "2021-07-23 10:03:04,097 INFO: | epoch   1 | step 4450 | batch 4450/11250 | lr 0.00006 0.00003 | loss 0.5141 | s/batch 0.23\n",
      "2021-07-23 10:03:16,680 INFO: | epoch   1 | step 4500 | batch 4500/11250 | lr 0.00006 0.00003 | loss 0.6095 | s/batch 0.25\n",
      "2021-07-23 10:03:28,996 INFO: | epoch   1 | step 4550 | batch 4550/11250 | lr 0.00006 0.00003 | loss 0.5395 | s/batch 0.25\n",
      "2021-07-23 10:03:42,094 INFO: | epoch   1 | step 4600 | batch 4600/11250 | lr 0.00006 0.00003 | loss 0.5431 | s/batch 0.26\n",
      "2021-07-23 10:03:54,778 INFO: | epoch   1 | step 4650 | batch 4650/11250 | lr 0.00006 0.00003 | loss 0.4919 | s/batch 0.25\n",
      "2021-07-23 10:04:07,493 INFO: | epoch   1 | step 4700 | batch 4700/11250 | lr 0.00006 0.00003 | loss 0.5912 | s/batch 0.25\n",
      "2021-07-23 10:04:19,703 INFO: | epoch   1 | step 4750 | batch 4750/11250 | lr 0.00006 0.00003 | loss 0.4922 | s/batch 0.24\n",
      "2021-07-23 10:04:32,189 INFO: | epoch   1 | step 4800 | batch 4800/11250 | lr 0.00006 0.00003 | loss 0.5339 | s/batch 0.25\n",
      "2021-07-23 10:04:43,157 INFO: | epoch   1 | step 4850 | batch 4850/11250 | lr 0.00006 0.00003 | loss 0.5202 | s/batch 0.22\n",
      "2021-07-23 10:04:56,327 INFO: | epoch   1 | step 4900 | batch 4900/11250 | lr 0.00006 0.00003 | loss 0.5414 | s/batch 0.26\n",
      "2021-07-23 10:05:08,543 INFO: | epoch   1 | step 4950 | batch 4950/11250 | lr 0.00006 0.00003 | loss 0.5354 | s/batch 0.24\n",
      "2021-07-23 10:05:19,601 INFO: | epoch   1 | step 5000 | batch 5000/11250 | lr 0.00005 0.00003 | loss 0.4879 | s/batch 0.22\n",
      "2021-07-23 10:05:31,314 INFO: | epoch   1 | step 5050 | batch 5050/11250 | lr 0.00005 0.00003 | loss 0.5251 | s/batch 0.23\n",
      "2021-07-23 10:05:44,064 INFO: | epoch   1 | step 5100 | batch 5100/11250 | lr 0.00005 0.00003 | loss 0.4600 | s/batch 0.25\n",
      "2021-07-23 10:05:55,843 INFO: | epoch   1 | step 5150 | batch 5150/11250 | lr 0.00005 0.00003 | loss 0.4179 | s/batch 0.24\n",
      "2021-07-23 10:06:09,295 INFO: | epoch   1 | step 5200 | batch 5200/11250 | lr 0.00005 0.00003 | loss 0.5758 | s/batch 0.27\n",
      "2021-07-23 10:06:20,735 INFO: | epoch   1 | step 5250 | batch 5250/11250 | lr 0.00005 0.00003 | loss 0.4975 | s/batch 0.23\n",
      "2021-07-23 10:06:33,341 INFO: | epoch   1 | step 5300 | batch 5300/11250 | lr 0.00005 0.00003 | loss 0.4444 | s/batch 0.25\n",
      "2021-07-23 10:06:47,452 INFO: | epoch   1 | step 5350 | batch 5350/11250 | lr 0.00005 0.00003 | loss 0.5216 | s/batch 0.28\n",
      "2021-07-23 10:06:59,810 INFO: | epoch   1 | step 5400 | batch 5400/11250 | lr 0.00005 0.00003 | loss 0.4944 | s/batch 0.25\n",
      "2021-07-23 10:07:12,489 INFO: | epoch   1 | step 5450 | batch 5450/11250 | lr 0.00005 0.00003 | loss 0.5364 | s/batch 0.25\n",
      "2021-07-23 10:07:25,079 INFO: | epoch   1 | step 5500 | batch 5500/11250 | lr 0.00005 0.00003 | loss 0.4587 | s/batch 0.25\n",
      "2021-07-23 10:07:37,404 INFO: | epoch   1 | step 5550 | batch 5550/11250 | lr 0.00005 0.00003 | loss 0.4111 | s/batch 0.25\n",
      "2021-07-23 10:07:49,510 INFO: | epoch   1 | step 5600 | batch 5600/11250 | lr 0.00005 0.00003 | loss 0.4640 | s/batch 0.24\n",
      "2021-07-23 10:08:01,324 INFO: | epoch   1 | step 5650 | batch 5650/11250 | lr 0.00005 0.00002 | loss 0.4952 | s/batch 0.24\n",
      "2021-07-23 10:08:14,862 INFO: | epoch   1 | step 5700 | batch 5700/11250 | lr 0.00005 0.00002 | loss 0.4819 | s/batch 0.27\n",
      "2021-07-23 10:08:26,616 INFO: | epoch   1 | step 5750 | batch 5750/11250 | lr 0.00005 0.00002 | loss 0.5028 | s/batch 0.24\n",
      "2021-07-23 10:08:40,218 INFO: | epoch   1 | step 5800 | batch 5800/11250 | lr 0.00005 0.00002 | loss 0.5109 | s/batch 0.27\n",
      "2021-07-23 10:08:53,868 INFO: | epoch   1 | step 5850 | batch 5850/11250 | lr 0.00005 0.00002 | loss 0.4552 | s/batch 0.27\n",
      "2021-07-23 10:09:05,837 INFO: | epoch   1 | step 5900 | batch 5900/11250 | lr 0.00005 0.00002 | loss 0.4479 | s/batch 0.24\n",
      "2021-07-23 10:09:16,945 INFO: | epoch   1 | step 5950 | batch 5950/11250 | lr 0.00005 0.00002 | loss 0.5009 | s/batch 0.22\n",
      "2021-07-23 10:09:29,000 INFO: | epoch   1 | step 6000 | batch 6000/11250 | lr 0.00004 0.00002 | loss 0.3760 | s/batch 0.24\n",
      "2021-07-23 10:09:41,622 INFO: | epoch   1 | step 6050 | batch 6050/11250 | lr 0.00004 0.00002 | loss 0.4469 | s/batch 0.25\n",
      "2021-07-23 10:09:52,650 INFO: | epoch   1 | step 6100 | batch 6100/11250 | lr 0.00004 0.00002 | loss 0.3915 | s/batch 0.22\n",
      "2021-07-23 10:10:06,026 INFO: | epoch   1 | step 6150 | batch 6150/11250 | lr 0.00004 0.00002 | loss 0.3802 | s/batch 0.27\n",
      "2021-07-23 10:10:19,143 INFO: | epoch   1 | step 6200 | batch 6200/11250 | lr 0.00004 0.00002 | loss 0.5111 | s/batch 0.26\n",
      "2021-07-23 10:10:32,655 INFO: | epoch   1 | step 6250 | batch 6250/11250 | lr 0.00004 0.00002 | loss 0.4362 | s/batch 0.27\n",
      "2021-07-23 10:10:45,260 INFO: | epoch   1 | step 6300 | batch 6300/11250 | lr 0.00004 0.00002 | loss 0.3958 | s/batch 0.25\n",
      "2021-07-23 10:10:55,946 INFO: | epoch   1 | step 6350 | batch 6350/11250 | lr 0.00004 0.00002 | loss 0.4118 | s/batch 0.21\n",
      "2021-07-23 10:11:07,725 INFO: | epoch   1 | step 6400 | batch 6400/11250 | lr 0.00004 0.00002 | loss 0.4275 | s/batch 0.24\n",
      "2021-07-23 10:11:21,068 INFO: | epoch   1 | step 6450 | batch 6450/11250 | lr 0.00004 0.00002 | loss 0.4108 | s/batch 0.27\n",
      "2021-07-23 10:11:34,486 INFO: | epoch   1 | step 6500 | batch 6500/11250 | lr 0.00004 0.00002 | loss 0.4229 | s/batch 0.27\n",
      "2021-07-23 10:11:46,682 INFO: | epoch   1 | step 6550 | batch 6550/11250 | lr 0.00004 0.00002 | loss 0.4450 | s/batch 0.24\n",
      "2021-07-23 10:11:58,285 INFO: | epoch   1 | step 6600 | batch 6600/11250 | lr 0.00004 0.00002 | loss 0.3965 | s/batch 0.23\n",
      "2021-07-23 10:12:10,663 INFO: | epoch   1 | step 6650 | batch 6650/11250 | lr 0.00004 0.00002 | loss 0.4385 | s/batch 0.25\n",
      "2021-07-23 10:12:24,915 INFO: | epoch   1 | step 6700 | batch 6700/11250 | lr 0.00004 0.00002 | loss 0.4135 | s/batch 0.29\n",
      "2021-07-23 10:12:36,456 INFO: | epoch   1 | step 6750 | batch 6750/11250 | lr 0.00004 0.00002 | loss 0.4338 | s/batch 0.23\n",
      "2021-07-23 10:12:47,562 INFO: | epoch   1 | step 6800 | batch 6800/11250 | lr 0.00004 0.00002 | loss 0.3917 | s/batch 0.22\n",
      "2021-07-23 10:13:01,612 INFO: | epoch   1 | step 6850 | batch 6850/11250 | lr 0.00004 0.00002 | loss 0.4556 | s/batch 0.28\n",
      "2021-07-23 10:13:13,926 INFO: | epoch   1 | step 6900 | batch 6900/11250 | lr 0.00004 0.00002 | loss 0.4010 | s/batch 0.25\n",
      "2021-07-23 10:13:27,367 INFO: | epoch   1 | step 6950 | batch 6950/11250 | lr 0.00004 0.00002 | loss 0.4531 | s/batch 0.27\n",
      "2021-07-23 10:13:41,185 INFO: | epoch   1 | step 7000 | batch 7000/11250 | lr 0.00003 0.00002 | loss 0.4528 | s/batch 0.28\n",
      "2021-07-23 10:13:55,314 INFO: | epoch   1 | step 7050 | batch 7050/11250 | lr 0.00003 0.00002 | loss 0.4229 | s/batch 0.28\n",
      "2021-07-23 10:14:08,586 INFO: | epoch   1 | step 7100 | batch 7100/11250 | lr 0.00003 0.00002 | loss 0.4571 | s/batch 0.27\n",
      "2021-07-23 10:14:21,186 INFO: | epoch   1 | step 7150 | batch 7150/11250 | lr 0.00003 0.00002 | loss 0.4584 | s/batch 0.25\n",
      "2021-07-23 10:14:33,618 INFO: | epoch   1 | step 7200 | batch 7200/11250 | lr 0.00003 0.00002 | loss 0.4154 | s/batch 0.25\n",
      "2021-07-23 10:14:45,098 INFO: | epoch   1 | step 7250 | batch 7250/11250 | lr 0.00003 0.00002 | loss 0.4310 | s/batch 0.23\n",
      "2021-07-23 10:14:56,941 INFO: | epoch   1 | step 7300 | batch 7300/11250 | lr 0.00003 0.00002 | loss 0.3882 | s/batch 0.24\n",
      "2021-07-23 10:15:10,500 INFO: | epoch   1 | step 7350 | batch 7350/11250 | lr 0.00003 0.00002 | loss 0.4748 | s/batch 0.27\n",
      "2021-07-23 10:15:24,318 INFO: | epoch   1 | step 7400 | batch 7400/11250 | lr 0.00003 0.00002 | loss 0.3776 | s/batch 0.28\n",
      "2021-07-23 10:15:35,472 INFO: | epoch   1 | step 7450 | batch 7450/11250 | lr 0.00003 0.00002 | loss 0.3727 | s/batch 0.22\n",
      "2021-07-23 10:15:47,985 INFO: | epoch   1 | step 7500 | batch 7500/11250 | lr 0.00003 0.00002 | loss 0.3756 | s/batch 0.25\n",
      "2021-07-23 10:16:00,768 INFO: | epoch   1 | step 7550 | batch 7550/11250 | lr 0.00003 0.00002 | loss 0.3498 | s/batch 0.26\n",
      "2021-07-23 10:16:13,615 INFO: | epoch   1 | step 7600 | batch 7600/11250 | lr 0.00003 0.00002 | loss 0.4164 | s/batch 0.26\n",
      "2021-07-23 10:16:26,732 INFO: | epoch   1 | step 7650 | batch 7650/11250 | lr 0.00003 0.00002 | loss 0.3410 | s/batch 0.26\n",
      "2021-07-23 10:16:39,903 INFO: | epoch   1 | step 7700 | batch 7700/11250 | lr 0.00003 0.00002 | loss 0.4348 | s/batch 0.26\n",
      "2021-07-23 10:16:52,928 INFO: | epoch   1 | step 7750 | batch 7750/11250 | lr 0.00003 0.00002 | loss 0.4159 | s/batch 0.26\n",
      "2021-07-23 10:17:04,491 INFO: | epoch   1 | step 7800 | batch 7800/11250 | lr 0.00003 0.00002 | loss 0.4009 | s/batch 0.23\n",
      "2021-07-23 10:17:16,887 INFO: | epoch   1 | step 7850 | batch 7850/11250 | lr 0.00003 0.00002 | loss 0.3753 | s/batch 0.25\n",
      "2021-07-23 10:17:30,869 INFO: | epoch   1 | step 7900 | batch 7900/11250 | lr 0.00003 0.00001 | loss 0.3581 | s/batch 0.28\n",
      "2021-07-23 10:17:43,210 INFO: | epoch   1 | step 7950 | batch 7950/11250 | lr 0.00003 0.00001 | loss 0.4533 | s/batch 0.25\n",
      "2021-07-23 10:17:55,390 INFO: | epoch   1 | step 8000 | batch 8000/11250 | lr 0.00002 0.00001 | loss 0.3683 | s/batch 0.24\n",
      "2021-07-23 10:18:07,890 INFO: | epoch   1 | step 8050 | batch 8050/11250 | lr 0.00002 0.00001 | loss 0.4583 | s/batch 0.25\n",
      "2021-07-23 10:18:20,167 INFO: | epoch   1 | step 8100 | batch 8100/11250 | lr 0.00002 0.00001 | loss 0.4137 | s/batch 0.25\n",
      "2021-07-23 10:18:34,274 INFO: | epoch   1 | step 8150 | batch 8150/11250 | lr 0.00002 0.00001 | loss 0.3953 | s/batch 0.28\n",
      "2021-07-23 10:18:46,916 INFO: | epoch   1 | step 8200 | batch 8200/11250 | lr 0.00002 0.00001 | loss 0.4776 | s/batch 0.25\n",
      "2021-07-23 10:18:58,439 INFO: | epoch   1 | step 8250 | batch 8250/11250 | lr 0.00002 0.00001 | loss 0.4140 | s/batch 0.23\n",
      "2021-07-23 10:19:09,494 INFO: | epoch   1 | step 8300 | batch 8300/11250 | lr 0.00002 0.00001 | loss 0.3629 | s/batch 0.22\n",
      "2021-07-23 10:19:21,097 INFO: | epoch   1 | step 8350 | batch 8350/11250 | lr 0.00002 0.00001 | loss 0.4080 | s/batch 0.23\n",
      "2021-07-23 10:19:34,792 INFO: | epoch   1 | step 8400 | batch 8400/11250 | lr 0.00002 0.00001 | loss 0.3859 | s/batch 0.27\n",
      "2021-07-23 10:19:46,977 INFO: | epoch   1 | step 8450 | batch 8450/11250 | lr 0.00002 0.00001 | loss 0.3857 | s/batch 0.24\n",
      "2021-07-23 10:20:00,127 INFO: | epoch   1 | step 8500 | batch 8500/11250 | lr 0.00002 0.00001 | loss 0.4425 | s/batch 0.26\n",
      "2021-07-23 10:20:13,179 INFO: | epoch   1 | step 8550 | batch 8550/11250 | lr 0.00002 0.00001 | loss 0.4165 | s/batch 0.26\n",
      "2021-07-23 10:20:25,283 INFO: | epoch   1 | step 8600 | batch 8600/11250 | lr 0.00002 0.00001 | loss 0.3767 | s/batch 0.24\n",
      "2021-07-23 10:20:37,871 INFO: | epoch   1 | step 8650 | batch 8650/11250 | lr 0.00002 0.00001 | loss 0.3630 | s/batch 0.25\n",
      "2021-07-23 10:20:52,407 INFO: | epoch   1 | step 8700 | batch 8700/11250 | lr 0.00002 0.00001 | loss 0.3980 | s/batch 0.29\n",
      "2021-07-23 10:21:08,335 INFO: | epoch   1 | step 8750 | batch 8750/11250 | lr 0.00002 0.00001 | loss 0.4012 | s/batch 0.32\n",
      "2021-07-23 10:21:20,986 INFO: | epoch   1 | step 8800 | batch 8800/11250 | lr 0.00002 0.00001 | loss 0.3172 | s/batch 0.25\n",
      "2021-07-23 10:21:32,519 INFO: | epoch   1 | step 8850 | batch 8850/11250 | lr 0.00002 0.00001 | loss 0.4601 | s/batch 0.23\n",
      "2021-07-23 10:21:43,930 INFO: | epoch   1 | step 8900 | batch 8900/11250 | lr 0.00002 0.00001 | loss 0.3740 | s/batch 0.23\n",
      "2021-07-23 10:21:57,624 INFO: | epoch   1 | step 8950 | batch 8950/11250 | lr 0.00002 0.00001 | loss 0.3359 | s/batch 0.27\n",
      "2021-07-23 10:22:09,641 INFO: | epoch   1 | step 9000 | batch 9000/11250 | lr 0.00002 0.00001 | loss 0.3867 | s/batch 0.24\n",
      "2021-07-23 10:22:22,327 INFO: | epoch   1 | step 9050 | batch 9050/11250 | lr 0.00002 0.00001 | loss 0.3989 | s/batch 0.25\n",
      "2021-07-23 10:22:36,020 INFO: | epoch   1 | step 9100 | batch 9100/11250 | lr 0.00002 0.00001 | loss 0.3498 | s/batch 0.27\n",
      "2021-07-23 10:22:49,037 INFO: | epoch   1 | step 9150 | batch 9150/11250 | lr 0.00002 0.00001 | loss 0.3348 | s/batch 0.26\n",
      "2021-07-23 10:23:01,483 INFO: | epoch   1 | step 9200 | batch 9200/11250 | lr 0.00002 0.00001 | loss 0.3388 | s/batch 0.25\n",
      "2021-07-23 10:23:14,702 INFO: | epoch   1 | step 9250 | batch 9250/11250 | lr 0.00002 0.00001 | loss 0.3260 | s/batch 0.26\n",
      "2021-07-23 10:23:25,255 INFO: | epoch   1 | step 9300 | batch 9300/11250 | lr 0.00002 0.00001 | loss 0.3770 | s/batch 0.21\n",
      "2021-07-23 10:23:39,471 INFO: | epoch   1 | step 9350 | batch 9350/11250 | lr 0.00002 0.00001 | loss 0.3935 | s/batch 0.28\n",
      "2021-07-23 10:23:52,422 INFO: | epoch   1 | step 9400 | batch 9400/11250 | lr 0.00002 0.00001 | loss 0.4040 | s/batch 0.26\n",
      "2021-07-23 10:24:04,421 INFO: | epoch   1 | step 9450 | batch 9450/11250 | lr 0.00002 0.00001 | loss 0.3626 | s/batch 0.24\n",
      "2021-07-23 10:24:20,116 INFO: | epoch   1 | step 9500 | batch 9500/11250 | lr 0.00002 0.00001 | loss 0.3803 | s/batch 0.31\n",
      "2021-07-23 10:24:32,345 INFO: | epoch   1 | step 9550 | batch 9550/11250 | lr 0.00002 0.00001 | loss 0.4422 | s/batch 0.24\n",
      "2021-07-23 10:24:44,789 INFO: | epoch   1 | step 9600 | batch 9600/11250 | lr 0.00002 0.00001 | loss 0.3597 | s/batch 0.25\n",
      "2021-07-23 10:24:57,016 INFO: | epoch   1 | step 9650 | batch 9650/11250 | lr 0.00002 0.00001 | loss 0.3837 | s/batch 0.24\n",
      "2021-07-23 10:25:10,686 INFO: | epoch   1 | step 9700 | batch 9700/11250 | lr 0.00002 0.00001 | loss 0.3876 | s/batch 0.27\n",
      "2021-07-23 10:25:25,124 INFO: | epoch   1 | step 9750 | batch 9750/11250 | lr 0.00002 0.00001 | loss 0.3605 | s/batch 0.29\n",
      "2021-07-23 10:25:37,577 INFO: | epoch   1 | step 9800 | batch 9800/11250 | lr 0.00002 0.00001 | loss 0.3805 | s/batch 0.25\n",
      "2021-07-23 10:25:49,393 INFO: | epoch   1 | step 9850 | batch 9850/11250 | lr 0.00002 0.00001 | loss 0.3708 | s/batch 0.24\n",
      "2021-07-23 10:26:02,357 INFO: | epoch   1 | step 9900 | batch 9900/11250 | lr 0.00002 0.00001 | loss 0.4678 | s/batch 0.26\n",
      "2021-07-23 10:26:15,186 INFO: | epoch   1 | step 9950 | batch 9950/11250 | lr 0.00002 0.00001 | loss 0.3633 | s/batch 0.26\n",
      "2021-07-23 10:26:28,106 INFO: | epoch   1 | step 10000 | batch 10000/11250 | lr 0.00001 0.00001 | loss 0.3312 | s/batch 0.26\n",
      "2021-07-23 10:26:40,892 INFO: | epoch   1 | step 10050 | batch 10050/11250 | lr 0.00001 0.00001 | loss 0.3839 | s/batch 0.26\n",
      "2021-07-23 10:26:51,877 INFO: | epoch   1 | step 10100 | batch 10100/11250 | lr 0.00001 0.00001 | loss 0.3521 | s/batch 0.22\n",
      "2021-07-23 10:27:05,067 INFO: | epoch   1 | step 10150 | batch 10150/11250 | lr 0.00001 0.00000 | loss 0.4601 | s/batch 0.26\n",
      "2021-07-23 10:27:18,386 INFO: | epoch   1 | step 10200 | batch 10200/11250 | lr 0.00001 0.00000 | loss 0.3691 | s/batch 0.27\n",
      "2021-07-23 10:27:31,898 INFO: | epoch   1 | step 10250 | batch 10250/11250 | lr 0.00001 0.00000 | loss 0.4487 | s/batch 0.27\n",
      "2021-07-23 10:27:45,574 INFO: | epoch   1 | step 10300 | batch 10300/11250 | lr 0.00001 0.00000 | loss 0.3809 | s/batch 0.27\n",
      "2021-07-23 10:28:00,173 INFO: | epoch   1 | step 10350 | batch 10350/11250 | lr 0.00001 0.00000 | loss 0.3121 | s/batch 0.29\n",
      "2021-07-23 10:28:12,020 INFO: | epoch   1 | step 10400 | batch 10400/11250 | lr 0.00001 0.00000 | loss 0.3668 | s/batch 0.24\n",
      "2021-07-23 10:28:23,926 INFO: | epoch   1 | step 10450 | batch 10450/11250 | lr 0.00001 0.00000 | loss 0.4331 | s/batch 0.24\n",
      "2021-07-23 10:28:36,605 INFO: | epoch   1 | step 10500 | batch 10500/11250 | lr 0.00001 0.00000 | loss 0.3834 | s/batch 0.25\n",
      "2021-07-23 10:28:49,576 INFO: | epoch   1 | step 10550 | batch 10550/11250 | lr 0.00001 0.00000 | loss 0.3509 | s/batch 0.26\n",
      "2021-07-23 10:29:02,352 INFO: | epoch   1 | step 10600 | batch 10600/11250 | lr 0.00001 0.00000 | loss 0.3650 | s/batch 0.26\n",
      "2021-07-23 10:29:15,446 INFO: | epoch   1 | step 10650 | batch 10650/11250 | lr 0.00001 0.00000 | loss 0.3856 | s/batch 0.26\n",
      "2021-07-23 10:29:26,973 INFO: | epoch   1 | step 10700 | batch 10700/11250 | lr 0.00001 0.00000 | loss 0.3095 | s/batch 0.23\n",
      "2021-07-23 10:29:42,040 INFO: | epoch   1 | step 10750 | batch 10750/11250 | lr 0.00001 0.00000 | loss 0.3778 | s/batch 0.30\n",
      "2021-07-23 10:29:55,834 INFO: | epoch   1 | step 10800 | batch 10800/11250 | lr 0.00001 0.00000 | loss 0.3950 | s/batch 0.28\n",
      "2021-07-23 10:30:09,462 INFO: | epoch   1 | step 10850 | batch 10850/11250 | lr 0.00001 0.00000 | loss 0.3861 | s/batch 0.27\n",
      "2021-07-23 10:30:20,509 INFO: | epoch   1 | step 10900 | batch 10900/11250 | lr 0.00001 0.00000 | loss 0.3001 | s/batch 0.22\n",
      "2021-07-23 10:30:31,634 INFO: | epoch   1 | step 10950 | batch 10950/11250 | lr 0.00001 0.00000 | loss 0.3668 | s/batch 0.22\n",
      "2021-07-23 10:30:44,853 INFO: | epoch   1 | step 11000 | batch 11000/11250 | lr 0.00001 0.00000 | loss 0.3170 | s/batch 0.26\n",
      "2021-07-23 10:30:57,524 INFO: | epoch   1 | step 11050 | batch 11050/11250 | lr 0.00001 0.00000 | loss 0.3421 | s/batch 0.25\n",
      "2021-07-23 10:31:09,482 INFO: | epoch   1 | step 11100 | batch 11100/11250 | lr 0.00001 0.00000 | loss 0.3538 | s/batch 0.24\n",
      "2021-07-23 10:31:22,976 INFO: | epoch   1 | step 11150 | batch 11150/11250 | lr 0.00001 0.00000 | loss 0.3972 | s/batch 0.27\n",
      "2021-07-23 10:31:35,001 INFO: | epoch   1 | step 11200 | batch 11200/11250 | lr 0.00001 0.00000 | loss 0.3402 | s/batch 0.24\n",
      "2021-07-23 10:31:46,513 INFO: | epoch   1 | step 11250 | batch 11250/11250 | lr 0.00001 0.00000 | loss 0.3865 | s/batch 0.23\n",
      "2021-07-23 10:31:46,726 INFO: | epoch   1 | score (76.26, 63.38, 67.19) | f1 67.19 | loss 0.6749 | time 2829.08\n",
      "2021-07-23 10:31:47,053 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.7766    0.8531    0.8131     35027\n",
      "          股票     0.8013    0.8876    0.8423     33251\n",
      "          体育     0.9068    0.9428    0.9244     28283\n",
      "          娱乐     0.7685    0.8401    0.8027     19920\n",
      "          时政     0.6828    0.6821    0.6824     13515\n",
      "          社会     0.6950    0.6593    0.6767     11009\n",
      "          教育     0.8214    0.7599    0.7894      8987\n",
      "          财经     0.7385    0.4614    0.5679      7957\n",
      "          家居     0.7231    0.6156    0.6650      7063\n",
      "          游戏     0.7743    0.5698    0.6565      5291\n",
      "          房产     0.7352    0.5449    0.6259      4428\n",
      "          时尚     0.6824    0.5078    0.5823      2818\n",
      "          彩票     0.7774    0.4027    0.5305      1639\n",
      "          星座     0.7933    0.1466    0.2474       812\n",
      "\n",
      "    accuracy                         0.7875    180000\n",
      "   macro avg     0.7626    0.6338    0.6719    180000\n",
      "weighted avg     0.7847    0.7875    0.7809    180000\n",
      "\n",
      "2021-07-23 10:35:50,241 INFO: | epoch   1 | dev | score (85.37, 82.09, 83.15) | f1 83.15 | time 243.19\n",
      "2021-07-23 10:35:50,278 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9214    0.9095    0.9154      3891\n",
      "          股票     0.8924    0.9299    0.9108      3694\n",
      "          体育     0.9762    0.9679    0.9720      3142\n",
      "          娱乐     0.9229    0.9413    0.9320      2213\n",
      "          时政     0.8442    0.8701    0.8570      1501\n",
      "          社会     0.8053    0.8626    0.8330      1223\n",
      "          教育     0.9136    0.9008    0.9072       998\n",
      "          财经     0.8702    0.6980    0.7746       884\n",
      "          家居     0.8609    0.7895    0.8237       784\n",
      "          游戏     0.8443    0.8501    0.8472       587\n",
      "          房产     0.7491    0.8130    0.7797       492\n",
      "          时尚     0.7571    0.7668    0.7619       313\n",
      "          彩票     0.8278    0.8187    0.8232       182\n",
      "          星座     0.7660    0.3750    0.5035        96\n",
      "\n",
      "    accuracy                         0.8959     20000\n",
      "   macro avg     0.8537    0.8209    0.8315     20000\n",
      "weighted avg     0.8962    0.8959    0.8951     20000\n",
      "\n",
      "2021-07-23 10:35:50,279 INFO: Exceed history dev = 0.00, current dev = 83.15\n",
      "/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set cuda\n",
    "gpu = 0\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bert_dataset_loader import get_examples, data_iter\n",
    "from bert_data import dev_data, test_data, train_data\n",
    "from bert_optimizer import Optimizer\n",
    "from bert_score import get_score, reformat\n",
    "import pandas as pd\n",
    "from bert_vocab import Vocab\n",
    "vocab = Vocab(train_data)\n",
    "from bert_module import Model\n",
    "model = Model(vocab)\n",
    "\n",
    "\n",
    "# build trainer\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clip = 5.0\n",
    "epochs = 1\n",
    "early_stops = 3\n",
    "log_interval = 50\n",
    "\n",
    "test_batch_size = 16\n",
    "train_batch_size = 16\n",
    "\n",
    "save_model = '/home/featurize/data/bert.bin'\n",
    "save_test = '/home/featurize/data/bert.csv'\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.report = True\n",
    "\n",
    "        self.train_data = get_examples(train_data, model.word_encoder, vocab)\n",
    "        self.dev_data = get_examples(dev_data, model.word_encoder, vocab)\n",
    "        self.test_data = get_examples(test_data, model.word_encoder, vocab)\n",
    "        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n",
    "\n",
    "        # criterion\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # label name\n",
    "        self.target_names = vocab.target_names\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)\n",
    "\n",
    "        # count\n",
    "        self.step = 0\n",
    "        self.early_stop = -1\n",
    "        self.best_train_f1, self.best_dev_f1 = 0, 0\n",
    "        self.last_epoch = epochs\n",
    "\n",
    "    def train(self):\n",
    "        logging.info('Start training...')\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_f1 = self._train(epoch)\n",
    "\n",
    "            dev_f1 = self._eval(epoch)\n",
    "\n",
    "            if self.best_dev_f1 <= dev_f1:\n",
    "                logging.info(\n",
    "                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n",
    "                torch.save(self.model.state_dict(), save_model)\n",
    "\n",
    "                self.best_train_f1 = train_f1\n",
    "                self.best_dev_f1 = dev_f1\n",
    "                self.early_stop = 0\n",
    "            else:\n",
    "                self.early_stop += 1\n",
    "                if self.early_stop == early_stops:\n",
    "                    logging.info(\n",
    "                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n",
    "                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n",
    "                    self.last_epoch = epoch\n",
    "                    break\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(torch.load(save_model))\n",
    "        self._eval(self.last_epoch + 1, test=True)\n",
    "\n",
    "    def _train(self, epoch):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        epoch_start_time = time.time()\n",
    "        overall_losses = 0\n",
    "        losses = 0\n",
    "        batch_idx = 1\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "\n",
    "            batch_outputs = self.model(batch_inputs)   # b X num_labels\n",
    "            loss = self.criterion(batch_outputs, batch_labels)  # softmax + 交叉熵\n",
    "            loss.backward()\n",
    "\n",
    "            loss_value = loss.detach().cpu().item()\n",
    "            losses += loss_value\n",
    "            overall_losses += loss_value\n",
    "\n",
    "            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "            y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n",
    "            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                lrs = self.optimizer.get_lr()\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n",
    "                        epoch, self.step, batch_idx, self.batch_num, lrs,\n",
    "                        losses / log_interval,\n",
    "                        elapsed / log_interval))\n",
    "\n",
    "                losses = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "        overall_losses /= self.batch_num\n",
    "        during_time = time.time() - epoch_start_time\n",
    "\n",
    "        # reformat\n",
    "        overall_losses = reformat(overall_losses, 4)\n",
    "        score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "        logging.info(\n",
    "            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                                  overall_losses,\n",
    "                                                                                  during_time))\n",
    "        if set(y_true) == set(y_pred) and self.report:\n",
    "            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "            logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def _eval(self, epoch, test=False):\n",
    "        self.model.eval()\n",
    "        start_time = time.time()\n",
    "        data = self.test_data if test else self.dev_data\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "\n",
    "                batch_outputs = self.model(batch_inputs)\n",
    "                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "                y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "            during_time = time.time() - start_time\n",
    "\n",
    "            if test:\n",
    "                df = pd.DataFrame({'label': y_pred})\n",
    "                df.to_csv(save_test, index=False, sep=',')\n",
    "            else:\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                                  during_time))\n",
    "                if set(y_true) == set(y_pred) and self.report:\n",
    "                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "                    logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def batch2tensor(self, batch_data):\n",
    "        '''\n",
    "            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n",
    "        '''\n",
    "        batch_size = len(batch_data)\n",
    "        doc_labels = []\n",
    "        doc_lens = []\n",
    "        doc_max_sent_len = []\n",
    "        for doc_data in batch_data:\n",
    "            doc_labels.append(doc_data[0])\n",
    "            doc_lens.append(doc_data[1])\n",
    "            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n",
    "            max_sent_len = max(sent_lens)\n",
    "            doc_max_sent_len.append(max_sent_len)\n",
    "\n",
    "        max_doc_len = max(doc_lens)\n",
    "        max_sent_len = max(doc_max_sent_len)\n",
    "\n",
    "        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n",
    "        batch_labels = torch.LongTensor(doc_labels)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for sent_idx in range(doc_lens[b]):\n",
    "                sent_data = batch_data[b][2][sent_idx]\n",
    "                for word_idx in range(sent_data[0]):\n",
    "                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n",
    "                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n",
    "                    batch_masks[b, sent_idx, word_idx] = 1\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_inputs1 = batch_inputs1.to(device)\n",
    "            batch_inputs2 = batch_inputs2.to(device)\n",
    "            batch_masks = batch_masks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels\n",
    "\n",
    "\n",
    "# train\n",
    "trainer = Trainer(model, vocab)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# test\n",
    "trainer.test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-binary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
